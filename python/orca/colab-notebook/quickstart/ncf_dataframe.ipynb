{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ncf_dataframe.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/intel-analytics/BigDL/blob/branch-2.0/python/orca/colab-notebook/quickstart/ncf_dataframe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsdUyI7vKEGF"
      },
      "source": [
        "\r\n",
        "![image.png](data:image/png;base64,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)\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQWPKGb4KNv6"
      },
      "source": [
        "## **Environment Preparation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwxHGCOQKToI"
      },
      "source": [
        "**Install Java 8**\r\n",
        "\r\n",
        "Run the cell on the **Google Colab** to install jdk 1.8.\r\n",
        "\r\n",
        "**Note:** if you run this notebook on your computer, root permission is required when running the cell to install Java 8. (You may ignore this cell if Java 8 has already been set up in your computer)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zV2cU3jJIXCg"
      },
      "source": [
        "# Install jdk8\r\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\r\n",
        "import os\r\n",
        "# Set environment variable JAVA_HOME.\r\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\r\n",
        "!update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java\r\n",
        "!java -version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDMnBeFSKz21"
      },
      "source": [
        "**Install BigDL**\r\n",
        "\r\n",
        "You can install the latest release version or latest pre-release version using `pip install --pre --upgrade bigdl-orca`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FKTQXbDYWWr"
      },
      "source": [
        "# Install latest release version of bigdl \r\n",
        "# Installing bigdl from pip will automatically install pyspark, bigdl, and their dependencies.\r\n",
        "!pip install --pre --upgrade bigdl-orca[ray]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zy-92WBjYdx5"
      },
      "source": [
        "## **Using Spark Dataframes for Distribtued Deep Learning** \r\n",
        "\r\n",
        "In this guide we will describe how to use Spark Dataframes to process large-scale dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAy37CZkYy3p"
      },
      "source": [
        "#### **Intialization** \r\n",
        "\r\n",
        "import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mNCKlZLY5xI"
      },
      "source": [
        "import os\r\n",
        "import zipfile\r\n",
        "import argparse\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "from bigdl.dllib.feature.dataset import base\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "from bigdl.orca import init_orca_context, stop_orca_context\r\n",
        "from bigdl.orca import OrcaContext\r\n",
        "from bigdl.orca.learn.tf2 import Estimator\r\n",
        "from bigdl.orca.data import SharedValue\r\n",
        "from pyspark.sql.functions import col\r\n",
        "import bigdl.orca.data.pandas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kU0wwMFgY9hs"
      },
      "source": [
        "## **Init Orca Context** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InTPPklsZMNW"
      },
      "source": [
        "# recommended to set it to True when running BigDL in Jupyter notebook \r\n",
        "OrcaContext.log_output = True # (this will display terminal's stdout and stderr in the Jupyter notebook).\r\n",
        "\r\n",
        "cluster_mode = \"local\"\r\n",
        "\r\n",
        "if cluster_mode == \"local\":  \r\n",
        "    init_orca_context(cluster_mode=\"local\", cores=1) # run in local mode\r\n",
        "elif cluster_mode == \"yarn\":  \r\n",
        "    init_orca_context(cluster_mode=\"yarn-client\", num_nodes=2, cores=2, driver_memory=\"6g\") # run on Hadoop YARN cluster"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dO2x4TI4ZRnq"
      },
      "source": [
        "## **Data Preprocessing with Spark Dataframes**\r\n",
        "\r\n",
        "Orca supports Spark Dataframes as the input to the distributed training, and as the input/output of the distributed inference. Consequently, the user can easily process large-scale dataset using Apache Spark, and directly apply AI models on the distributed (and possibly in-memory) Dataframes without data conversion or serialization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xyxitSDZP8V"
      },
      "source": [
        "# Download and extract movielens 1M data.\r\n",
        "url = 'http://files.grouplens.org/datasets/movielens/ml-1m.zip'\r\n",
        "local_file = base.maybe_download('ml-1m.zip', '.', url)\r\n",
        "if not os.path.exists('./ml-1m'):\r\n",
        "        zip_ref = zipfile.ZipFile(local_file, 'r')\r\n",
        "        zip_ref.extractall('.')\r\n",
        "        zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxeY0f2Ia7yo"
      },
      "source": [
        "# Read in the dataset, and do a little preprocessing\r\n",
        "rating_files=\"./ml-1m/ratings.dat\"\r\n",
        "new_rating_files=\"./ml-1m/ratings_new.dat\"\r\n",
        "if not os.path.exists(new_rating_files):\r\n",
        "        fin = open(rating_files, \"rt\")\r\n",
        "        fout = open(new_rating_files, \"wt\")\r\n",
        "        for line in fin:\r\n",
        "            # replace :: to : for spark 2.4 support\r\n",
        "            fout.write(line.replace('::', ':'))\r\n",
        "        fin.close()\r\n",
        "        fout.close()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BVhLa-ebFTe",
        "outputId": "413d2b62-d9ab-4f37-f610-6e7273f9512a"
      },
      "source": [
        "# read csv\r\n",
        "spark = OrcaContext.get_spark_session()\r\n",
        "df = spark.read.csv(new_rating_files, sep=':', header=True, inferSchema=True).toDF(\r\n",
        "  \"user\", \"item\", \"label\", \"timestamp\")\r\n",
        "\r\n",
        "user_set = df.select('user').collect()\r\n",
        "item_set = df.select('item').collect()\r\n",
        "\r\n",
        "min_user_id = min(user_set)[0]\r\n",
        "max_user_id = max(user_set)[0]\r\n",
        "min_item_id = min(item_set)[0]\r\n",
        "max_item_id = max(item_set)[0]\r\n",
        "print(min_user_id, max_user_id, min_item_id, max_item_id)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1 6040 1 3952\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFLTccOlbTaa"
      },
      "source": [
        "# update label starting from 0\r\n",
        "df = df.withColumn('label', df.label-1)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXym4BodbzwS"
      },
      "source": [
        "# split to train/test dataset\r\n",
        "train_data, test_data = df.randomSplit([0.8, 0.2], 100)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEgxpdYcb7oC"
      },
      "source": [
        "### **Define NCF Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnPGNXK0b8ot"
      },
      "source": [
        "def model_creator(config):\r\n",
        "    import tensorflow as tf\r\n",
        "    from tensorflow import keras\r\n",
        "    embedding_size=16\r\n",
        "    user = keras.layers.Input(dtype=tf.int32, shape=(None,))\r\n",
        "    item = keras.layers.Input(dtype=tf.int32, shape=(None,))\r\n",
        "    label = keras.layers.Input(dtype=tf.int32, shape=(None,))\r\n",
        "\r\n",
        "    with tf.name_scope(\"GMF\"):\r\n",
        "        user_embed_GMF = keras.layers.Embedding(max_user_id + 1, embedding_size)(user)\r\n",
        "        item_embed_GMF = keras.layers.Embedding(max_item_id + 1, embedding_size)(item)\r\n",
        "        GMF = keras.layers.Multiply()([user_embed_GMF, item_embed_GMF])\r\n",
        "\r\n",
        "    with tf.name_scope(\"MLP\"):\r\n",
        "        user_embed_MLP = keras.layers.Embedding(max_user_id + 1, embedding_size)(user)\r\n",
        "        item_embed_MLP = keras.layers.Embedding(max_item_id + 1, embedding_size)(item)\r\n",
        "        interaction = tf.concat([user_embed_MLP, item_embed_MLP], axis=-1)\r\n",
        "        layer1_MLP = keras.layers.Dense(units=embedding_size * 2, activation='relu')(interaction)\r\n",
        "        layer1_MLP = keras.layers.Dropout(rate=0.2)(layer1_MLP)\r\n",
        "        layer2_MLP = keras.layers.Dense(units=embedding_size, activation='relu')(layer1_MLP)\r\n",
        "        layer2_MLP = keras.layers.Dropout(rate=0.2)(layer2_MLP)\r\n",
        "        layer3_MLP = keras.layers.Dense(units=embedding_size // 2, activation='relu')(layer2_MLP)\r\n",
        "        layer3_MLP = keras.layers.Dropout(rate=0.2)(layer3_MLP)\r\n",
        "\r\n",
        "    # Concate the two parts together\r\n",
        "    with tf.name_scope(\"concatenation\"):\r\n",
        "        concatenation = tf.concat([GMF, layer3_MLP], axis=-1)\r\n",
        "        outputs = keras.layers.Dense(units=5, activation='softmax')(concatenation)\r\n",
        "    \r\n",
        "    model = keras.Model(inputs=[user, item], outputs=outputs)\r\n",
        "    model.compile(optimizer= \"adam\",\r\n",
        "                  loss= \"sparse_categorical_crossentropy\",\r\n",
        "                  metrics=['accuracy'])\r\n",
        "    return model"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1srWZc5DcHMG"
      },
      "source": [
        "### **Fit with Orca Estimator**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqTuXJbYcRj6"
      },
      "source": [
        "batch_size=1280\r\n",
        "epochs=2\r\n",
        "model_dir='./'\r\n",
        "\r\n",
        "# create an Estimator\r\n",
        "est = Estimator.from_keras(model_creator=model_creator, workers_per_node=1)\r\n",
        "\r\n",
        "stats = est.fit(train_data,\r\n",
        "                epochs=epochs,\r\n",
        "                batch_size=batch_size,\r\n",
        "                feature_cols=['user', 'item'],\r\n",
        "                label_cols=['label'],\r\n",
        "                steps_per_epoch=800000 // batch_size,\r\n",
        "                validation_data=test_data,\r\n",
        "                validation_steps = 200000 // batch_size)\r\n",
        "\r\n",
        "checkpoint_path = os.path.join(model_dir, \"NCF.ckpt\")\r\n",
        "est.save(checkpoint_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-ikO_Da3qHv"
      },
      "source": [
        "# evaluate with Estimator\r\n",
        "stats = est.evaluate(test_data, \r\n",
        "                     feature_cols=['user', 'item'],\r\n",
        "                     label_cols=['label'],\r\n",
        "                     num_steps=100000 // batch_size)\r\n",
        "est.shutdown()\r\n",
        "print(stats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeJsTahMgFxE"
      },
      "source": [
        "stop_orca_context()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
