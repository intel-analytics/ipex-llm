{"cells":[{"cell_type":"code","source":["import math\nimport argparse\nimport os\nimport random\n\nfrom bigdl.orca import init_orca_context, stop_orca_context\nfrom bigdl.orca.learn.tf2.estimator import Estimator\nfrom pyspark.sql.types import StructType, StructField, IntegerType\nfrom bigdl.orca import OrcaContext\n\n\ndef build_model(num_users, num_items, class_num, layers=[20, 10], include_mf=True, mf_embed=20):\n    import tensorflow as tf\n    from tensorflow.keras.layers import Input, Embedding, Dense, Flatten, concatenate, multiply\n\n    num_layer = len(layers)\n    user_input = Input(shape=(1,), dtype='int32', name='user_input')\n    item_input = Input(shape=(1,), dtype='int32', name='item_input')\n\n    mlp_embed_user = Embedding(input_dim=num_users, output_dim=int(layers[0] / 2),\n                               input_length=1)(user_input)\n    mlp_embed_item = Embedding(input_dim=num_items, output_dim=int(layers[0] / 2),\n                               input_length=1)(item_input)\n\n    user_latent = Flatten()(mlp_embed_user)\n    item_latent = Flatten()(mlp_embed_item)\n\n    mlp_latent = concatenate([user_latent, item_latent], axis=1)\n    for idx in range(1, num_layer):\n        layer = Dense(layers[idx], activation='relu',\n                      name='layer%d' % idx)\n        mlp_latent = layer(mlp_latent)\n\n    if include_mf:\n        mf_embed_user = Embedding(input_dim=num_users,\n                                  output_dim=mf_embed,\n                                  input_length=1)(user_input)\n        mf_embed_item = Embedding(input_dim=num_users,\n                                  output_dim=mf_embed,\n                                  input_length=1)(item_input)\n        mf_user_flatten = Flatten()(mf_embed_user)\n        mf_item_flatten = Flatten()(mf_embed_item)\n\n        mf_latent = multiply([mf_user_flatten, mf_item_flatten])\n        concated_model = concatenate([mlp_latent, mf_latent], axis=1)\n        prediction = Dense(class_num, activation='softmax', name='prediction')(concated_model)\n    else:\n        prediction = Dense(class_num, activation='softmax', name='prediction')(mlp_latent)\n\n    model = tf.keras.Model([user_input, item_input], prediction)\n    return model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9c1d63ee-fedd-442e-809c-866e7ebfcbb3"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["executor_cores = 2\nlr = 0.001\nepochs = 5\nbatch_size = 8000\nbackend = \"ray\" # ray or spark\ndata_dir = './'\nmodel_dir = \"/dbfs/FileStore/model/ncf_train/\"\nsave_path = model_dir + \"ncf.h5\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d5e937f2-b9c2-4665-9d66-ec4a2e1d99e7"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["sc = init_orca_context(cluster_mode=\"spark-submit\")\n\nspark = OrcaContext.get_spark_session()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6a7defe0-1fff-45fa-9bfe-734ece07a627"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# To make things simple, we are just generating some dummy data for this example.\nnum_users, num_items = 6000, 3000\nrdd = sc.range(0, 50000).map(\n    lambda x: [random.randint(0, num_users - 1), random.randint(0, num_items - 1), random.randint(0, 4)])\nschema = StructType([StructField(\"user\", IntegerType(), False),\n                     StructField(\"item\", IntegerType(), False),\n                     StructField(\"label\", IntegerType(), False)])\ndata = spark.createDataFrame(rdd, schema)\ntrain, test = data.randomSplit([0.8, 0.2], seed=1)\n\nconfig = {\"lr\": lr, \"inter_op_parallelism\": 4, \"intra_op_parallelism\": executor_cores}\n\n\ndef model_creator(config):\n    import tensorflow as tf\n\n    model = build_model(num_users, num_items, 5)\n    print(model.summary())\n    optimizer = tf.keras.optimizers.Adam(config[\"lr\"])\n    model.compile(optimizer=optimizer,\n                  loss='sparse_categorical_crossentropy',\n                  metrics=['sparse_categorical_crossentropy', 'accuracy'])\n    return model\n\n\nsteps_per_epoch = math.ceil(train.count() / batch_size)\nval_steps = math.ceil(test.count() / batch_size)\n\nestimator = Estimator.from_keras(model_creator=model_creator,\n                                 verbose=False,\n                                 config=config,\n                                 backend=backend,\n                                 model_dir=model_dir)\nestimator.fit(train,\n              batch_size=batch_size,\n              epochs=epochs,\n              feature_cols=['user', 'item'],\n              label_cols=['label'],\n              steps_per_epoch=steps_per_epoch,\n              validation_data=test,\n              validation_steps=val_steps)\n\npredictions = estimator.predict(test,\n                                batch_size=batch_size,\n                                feature_cols=['user', 'item'],\n                                steps=val_steps)\nprint(\"Predictions on validation dataset:\")\npredictions.show(5, truncate=False)\n\nprint(\"Saving model to: \", save_path)\nestimator.save(save_path)\n\n# load with estimator.load(save_path)\n# estimator.load(save_path)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"25052ba9-ac56-4d31-9185-6ede2ba70659"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["stop_orca_context()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8428c199-1418-4fd4-91b9-8eb195434a5c"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"tf_keras_ncf","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":3351030297539041}},"nbformat":4,"nbformat_minor":0}
