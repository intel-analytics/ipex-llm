{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dllib_keras_api.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dding3/BigDL/blob/addoutput/python/dllib/colab-notebook/dllib_keras_api.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgV_QLGE9Lox"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDBPZ0_rfBmU"
      },
      "source": [
        "##### Copyright 2016 The BigDL Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBWVU_bhfkY7"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "#"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voMBntim9bMf"
      },
      "source": [
        "## **Environment Preparation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_OS4HKJMNpv"
      },
      "source": [
        "**Install bigdl-dllib**\n",
        "\n",
        "You can install the latest pre-release version using `pip install --pre --upgrade bigdl-dllib`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qfT8CaC51hI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af536b5a-fe5a-4277-8f9c-88e80d347af1"
      },
      "source": [
        "# Install latest pre-release version of bigdl-dllib with spark3\n",
        "# Find the latest bigdl-dllib with spark3 from https://sourceforge.net/projects/analytics-zoo/files/dllib-py-spark3/ and intall it\n",
        "!pip install https://sourceforge.net/projects/analytics-zoo/files/dllib-py-spark3/bigdl_dllib_spark3-0.14.0b20211107-py3-none-manylinux1_x86_64.whl\n",
        "\n",
        "exit() # restart the runtime to refresh installed pkg"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bigdl-dllib-spark3==0.14.0b20211107\n",
            "  Downloading https://sourceforge.net/projects/analytics-zoo/files/dllib-py-spark3/bigdl_dllib_spark3-0.14.0b20211107-py3-none-manylinux1_x86_64.whl (93.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 93.9 MB 10 kB/s \n",
            "\u001b[?25hCollecting pyspark==3.1.2\n",
            "  Downloading pyspark-3.1.2.tar.gz (212.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 212.4 MB 56 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from bigdl-dllib-spark3==0.14.0b20211107) (1.19.5)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from bigdl-dllib-spark3==0.14.0b20211107) (1.15.0)\n",
            "Collecting py4j==0.10.9\n",
            "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 51.8 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.2-py2.py3-none-any.whl size=212880768 sha256=12522ef0b9b3e431f5fa73999c4a989ad87b4706508a1e68904ec038407dd3e4\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/0a/c1/9561f6fecb759579a7d863dcd846daaa95f598744e71b02c77\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark, bigdl-dllib-spark3\n",
            "Successfully installed bigdl-dllib-spark3-0.14.0b20211107 py4j-0.10.9 pyspark-3.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-l4vel5N3qP"
      },
      "source": [
        "## **Distributed NN model using DLlib keras-style api for classification**\n",
        "\n",
        "In this guide we will demonstrate how to use dllib keras-style api to build a model for classification in 5 simple steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84cTfA0b8xL6"
      },
      "source": [
        "#### **Step 0: Intialization** \n",
        "\n",
        "import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ukzsLiu886t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2c5f835-0b3d-4b13-83fc-f27d2a9e4068"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import DoubleType\n",
        "from pyspark.sql.functions import lit\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "from bigdl.dllib.nncontext import *\n",
        "from bigdl.dllib.keras.layers import *\n",
        "from bigdl.dllib.keras.model import *\n",
        "from bigdl.dllib.nnframes import *\n",
        "from bigdl.dllib.nn.criterion import *"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepending /usr/local/lib/python3.7/dist-packages/bigdl/share/dllib/conf/spark-bigdl.conf to sys.path\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1i_mMkHEKLyk"
      },
      "source": [
        "Init NNContext and create Spark session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKXOL7yP9Oeq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39d7d75e-6c7f-418a-b696-ca4f2565f2e2"
      },
      "source": [
        "sc = init_nncontext(cluster_mode=\"local\") # run in local mode\n",
        "spark = SparkSession(sc)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current pyspark location is : /usr/local/lib/python3.7/dist-packages/pyspark/__init__.py\n",
            "Start to getOrCreate SparkContext\n",
            "pyspark_submit_args is:  --driver-class-path /usr/local/lib/python3.7/dist-packages/bigdl/share/dllib/lib/bigdl-dllib-spark_3.1.2-0.14.0-SNAPSHOT-jar-with-dependencies.jar pyspark-shell \n",
            "Successfully got a SparkContext\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsT-0y8w-6N5"
      },
      "source": [
        "#### **Step 1: Load the data** \n",
        "\n",
        "We used [Pima Indians onset of diabetes](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv) as dataset for the demo. It's a standard machine learning dataset from the UCI Machine Learning repository. It describes patient medical record data for Pima Indians and whether they had an onset of diabetes within five years.    \n",
        "\n",
        "For more details about the data, please refer [here](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.names)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYhkTLkcLMvX"
      },
      "source": [
        "##### ***Download the data***\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "devxHuDW-0Mb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "394b7650-b892-4ac7-fad6-10d633e9ff0e"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-08 06:18:16--  https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 23278 (23K) [text/plain]\n",
            "Saving to: ‘pima-indians-diabetes.data.csv’\n",
            "\n",
            "\r          pima-indi   0%[                    ]       0  --.-KB/s               \rpima-indians-diabet 100%[===================>]  22.73K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2021-11-08 06:18:16 (9.86 MB/s) - ‘pima-indians-diabetes.data.csv’ saved [23278/23278]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bni-rogFMQVV"
      },
      "source": [
        "\n",
        "DLlib supports Spark Dataframes as the input to the distributed training, and as the input/output of the distributed inference. Consequently, the user can easily process large-scale dataset using Apache Spark, and directly apply AI models on the distributed (and possibly in-memory) Dataframes without data conversion or serialization   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfwvZDqVQDEh"
      },
      "source": [
        "##### **Load the data into Spark dataframe using Spark API**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVpFKkCX_3WF"
      },
      "source": [
        "path = \"pima-indians-diabetes.data.csv\"\n",
        "df = spark.read.csv(path, sep=',', inferSchema=True).toDF(\"num_times_pregrant\", \"plasma_glucose\", \"blood_pressure\", \"skin_fold_thickness\", \"2-hour_insulin\", \"body_mass_index\", \"diabetes_pedigree_function\", \"age\", \"class\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68UHw7AK-ouh"
      },
      "source": [
        "#### **Step 2: Process data using Spark DataFrame api** \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_15SCI5_-LPi"
      },
      "source": [
        "vecAssembler = VectorAssembler(outputCol=\"features\")\n",
        "vecAssembler.setInputCols([\"num_times_pregrant\", \"plasma_glucose\", \"blood_pressure\", \"skin_fold_thickness\", \"2-hour_insulin\", \"body_mass_index\", \"diabetes_pedigree_function\", \"age\"])\n",
        "train_df = vecAssembler.transform(df)\n",
        "\n",
        "changedTypedf = train_df.withColumn(\"label\", train_df[\"class\"].cast(DoubleType())+lit(1))\\\n",
        "    .select(\"features\", \"label\")\n",
        "(trainingDF, validationDF) = changedTypedf.randomSplit([0.9, 0.1])\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ld-pV8e-Vb7D"
      },
      "source": [
        "#### **Step 3: Define model using DLlib keras-style api** \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAMxwh-KBrdt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cac6832a-24c1-4526-c70a-dec335a59cd1"
      },
      "source": [
        "x1 = Input(shape=(8,))\n",
        "dense1 = Dense(12, activation='relu')(x1)\n",
        "dense2 = Dense(8, activation='relu')(dense1)\n",
        "dense3 = Dense(2)(dense2)\n",
        "model = Model(x1, dense3)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "creating: createZooKerasInput\n",
            "creating: createZooKerasDense\n",
            "creating: createZooKerasDense\n",
            "creating: createZooKerasDense\n",
            "creating: createZooKerasModel\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kEZlexfMm0z"
      },
      "source": [
        "#### **Step 4: Create NNClassifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bhoBcE1Mbn6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "808bbdba-a7bd-42af-b3a2-d646a6c41869"
      },
      "source": [
        "classifier = NNClassifier(model, CrossEntropyCriterion(), [8]) \\\n",
        "    .setOptimMethod(Adam()) \\\n",
        "    .setBatchSize(32) \\\n",
        "    .setMaxEpoch(150)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "creating: createCrossEntropyCriterion\n",
            "creating: createScalarToTensor\n",
            "creating: createSeqToTensor\n",
            "creating: createFeatureLabelPreprocessing\n",
            "creating: createNNClassifier\n",
            "creating: createAdam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzgamNMmNPV3"
      },
      "source": [
        "Train the model with Spark dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4x-S30kNSRC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a64177f5-4fcd-4aae-c74d-24c6f3392a1d"
      },
      "source": [
        "nnModel = classifier.fit(trainingDF)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "creating: createToTuple\n",
            "creating: createChainedPreprocessing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIEJ6wgvNbWv"
      },
      "source": [
        "#### **Step 5: Evaluate the trained model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mv8e3JwhNhcS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8bfaaa0-eafa-4727-b3fb-6da55c5583ad"
      },
      "source": [
        "predictionDF = nnModel.transform(validationDF).cache()\n",
        "predictionDF.sample(False, 0.1).show()\n",
        "\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictionDF)\n",
        "\n",
        "print(\"Accuracy = %g \" % (accuracy))\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+----------+\n",
            "|            features|label|prediction|\n",
            "+--------------------+-----+----------+\n",
            "|[0.0,121.0,66.0,3...|  2.0|       1.0|\n",
            "|[1.0,73.0,50.0,10...|  1.0|       1.0|\n",
            "|[1.0,81.0,72.0,18...|  1.0|       1.0|\n",
            "|[1.0,93.0,56.0,11...|  1.0|       1.0|\n",
            "|[1.0,149.0,68.0,2...|  2.0|       1.0|\n",
            "|[2.0,112.0,86.0,4...|  1.0|       1.0|\n",
            "|[3.0,96.0,56.0,34...|  1.0|       1.0|\n",
            "|[4.0,151.0,90.0,3...|  1.0|       1.0|\n",
            "|[6.0,162.0,62.0,0...|  2.0|       1.0|\n",
            "|[7.0,107.0,74.0,0...|  2.0|       1.0|\n",
            "|[8.0,181.0,68.0,3...|  2.0|       2.0|\n",
            "|[14.0,175.0,62.0,...|  2.0|       2.0|\n",
            "+--------------------+-----+----------+\n",
            "\n",
            "Accuracy = 0.690141 \n"
          ]
        }
      ]
    }
  ]
}