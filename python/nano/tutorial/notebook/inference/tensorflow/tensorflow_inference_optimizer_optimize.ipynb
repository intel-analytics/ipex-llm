{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[View the runnable example on GitHub](https://github.com/intel-analytics/BigDL/tree/main/python/nano/tutorial/notebook/inference/tensorflow/tensorflow_inference_optimizer_optimize.ipynb)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Find Acceleration Method with the Minimum Inference Latency for TensorFlow model using InferenceOptimizer"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "This example illustrates how to apply InferenceOptimizer to quickly find acceleration method with the minimum inference latency for Tensorflow model under specific restrictions or without restrictions for a trained model. By calling `optimize()`, we can obtain all available accelaration combinations provided by BigDL-Nano for inference. By calling `get_best_model()` , we could get the best model under specific restrictions or without restrictions."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "To inference using BigDL-Nano InferenceOptimizer, the following packages need to be installed first. We recommend you to use [Miniconda](https://docs.conda.io/en/latest/miniconda.html) to prepare the environment and install the following packages in a conda environment. \n",
        "\n",
        "You can create a conda environment by executing:\n",
        "\n",
        "```bash\n",
        "# \"nano\" is conda environment name, you can use any name you like.\n",
        "conda create -n nano python=3.7 setuptools=58.0.4  \n",
        "conda activate nano\n",
        "pip install --pre --upgrade bigdl-nano[tensorflow,inference]\n",
        "```\n"
      ],
      "metadata": {
         "nbsphinx": "hidden"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then initialize environment variables with script `bigdl-nano-init` installed with bigdl-nano.\n",
        "\n",
        "```bash\n",
        "source bigdl-nano-init\n",
        "```"
      ],
      "metadata": {
         "nbsphinx": "hidden"
     }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, prepare model and dataset. We use a pretrained [EfficientNetB0 model](https://www.tensorflow.org/api_docs/python/tf/keras/applications/efficientnet/EfficientNetB0) on Imagenet dataset and train the model on  on [Imagenette](https://www.tensorflow.org/datasets/catalog/imagenette) in this example."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from bigdl.nano.tf.keras import Model\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tqdm import tqdm\n",
        "\n",
        "def prepare_dataset(img_size=224, batch_size=512):\n",
        "    (ds_train, ds_test), ds_info = tfds.load(\n",
        "        'imagenette/320px-v2',\n",
        "        data_dir=\"./data/\",\n",
        "        split=['train', 'validation'],\n",
        "        with_info=True,\n",
        "        as_supervised=True\n",
        "    )\n",
        "\n",
        "    num_classes = ds_info.features['label'].num_classes\n",
        "\n",
        "    def preprocessing(input_img, label):\n",
        "        return tf.image.resize(input_img, (img_size, img_size)), tf.one_hot(label, num_classes)\n",
        "\n",
        "    AUTOTUNE = tf.data.AUTOTUNE\n",
        "    ds_train_batched = ds_train.shuffle(1000).map(preprocessing).batch(batch_size, drop_remainder=False).prefetch(AUTOTUNE)\n",
        "    ds_test_batched = ds_test.map(preprocessing).batch(batch_size, drop_remainder=False).prefetch(AUTOTUNE)\n",
        "    calib_set = ds_train.map(preprocessing).prefetch(AUTOTUNE)\n",
        "\n",
        "    for img_t, lbl_t in tqdm(ds_test_batched):\n",
        "        valid_data = tf.data.Dataset.from_tensor_slices((img_t, lbl_t))\n",
        "        break\n",
        "\n",
        "    return ds_train_batched, ds_test_batched, calib_set, valid_data, ds_info\n",
        "\n",
        "def create_model(num_classes=10, img_size=224):\n",
        "    inputs = layers.Input(shape=(img_size, img_size, 3))\n",
        "    x = tf.cast(inputs, tf.float32)\n",
        "    backbone = EfficientNetB0(weights='imagenet', include_top=False)\n",
        "    backbone.trainable = False\n",
        "    x = backbone(x)\n",
        "    x = layers.GlobalAveragePooling2D('channels_last')(x)\n",
        "    x = layers.Dense(512, activation='relu')(x)\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
        "    return model"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "nbsphinx": "hidden"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set, test_set, calibration_set, validation_set, ds_info = prepare_dataset()\n",
        "ori_model = create_model()\n",
        "ori_model.fit(train_set,\n",
        "          epochs=5,\n",
        "          steps_per_epoch=(ds_info.splits['train'].num_examples // 512 + 1),\n",
        "          )"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;_The full definition of function_ `prepare_dataset` _and_ `create_model` _ could be found in the_ [runnable example](https://github.com/intel-analytics/BigDL/tree/main/python/nano/tutorial/notebook/inference/tensorflow/tensorflow_inference_optimizer_optimize.ipynb)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Obtain available accelaration combinations by `optimize`"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Default search mode\n",
        "To find acceleration method with the minimum inference latency, you could import `InferenceOptimizer` and call `optimize` method. The `optimize` method will run all possible acceleration combinations and output the result."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from bigdl.nano.tf.keras import InferenceOptimizer\n",
        "\n",
        "opt = InferenceOptimizer()\n",
        "opt.optimize(ori_model,\n",
        "             x=calibration_set,\n",
        "             latency_sample_num=10)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The example output of `optimizer.optimize` is shown below.\n",
        "\n",
        "```bash\n",
        "==========================Optimization Results==========================\n",
        " -------------------------------- ---------------------- --------------\n",
        "|             method             |        status        | latency(ms)  |\n",
        " -------------------------------- ---------------------- --------------\n",
        "|            original            |      successful      |   110.198    |\n",
        "|              int8              |      successful      |    55.621    |\n",
        "|         openvino_fp32          |      successful      |    30.763    |\n",
        "|         openvino_int8          |      successful      |    33.872    |\n",
        "|        onnxruntime_fp32        |      successful      |    23.38     |\n",
        "|    onnxruntime_int8_qlinear    |      successful      |    9.836     |\n",
        "|    onnxruntime_int8_integer    |      successful      |    12.899    |\n",
        " -------------------------------- ---------------------- --------------\n",
        "Optimization cost 347.9s in total.\n",
        "```"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Search with accuracy supervision\n",
        "When calling `optimize`, to care about the possible accuracy drop, you could specify `validation_data`, `metric`, `direction` paramaters to enable validation:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.metrics import CategoricalAccuracy\n",
        "\n",
        "opt.optimize(ori_model,\n",
        "             x=calibration_set,\n",
        "             validation_data=validation_set,\n",
        "             metric=CategoricalAccuracy(),\n",
        "             direction=\"max\",\n",
        "             latency_sample_num=10)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The example output of `optimizer.optimize` is shown below.\n",
        "\n",
        "```bash\n",
        "==========================Optimization Results==========================\n",
        " -------------------------------- ---------------------- -------------- ----------------------\n",
        "|             method             |        status        | latency(ms)  |     metric value     |\n",
        " -------------------------------- ---------------------- -------------- ----------------------\n",
        "|            original            |      successful      |   106.692    |        0.996         |\n",
        "|              int8              |      successful      |    55.652    |        0.996         |\n",
        "|         openvino_fp32          |      successful      |    32.002    |        0.996*        |\n",
        "|         openvino_int8          |      successful      |    33.648    |        0.995         |\n",
        "|        onnxruntime_fp32        |      successful      |    25.639    |        0.996*        |\n",
        "|    onnxruntime_int8_qlinear    |      successful      |    9.877     |        0.971         |\n",
        "|    onnxruntime_int8_integer    |      successful      |     9.85     |        0.956         |\n",
        " -------------------------------- ---------------------- -------------- ----------------------\n",
        "* means we assume the metric value of the traced model does not change, so we don't recompute metric value to save time.\n",
        "Optimization cost 465.5s in total.\n",
        "```"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Filter acceleration methods\n",
        "In some cases, you may just want to test or compare several specific methods, there are two ways to achieve this.\n",
        "\n",
        "1. If you just want to test very little methods, you could just set `includes` parameter:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.metrics import CategoricalAccuracy\n",
        "\n",
        "opt.optimize(ori_model,\n",
        "             x=calibration_set,\n",
        "             validation_data=validation_set,\n",
        "             metric=CategoricalAccuracy(),\n",
        "             direction=\"max\",\n",
        "             includes=[\"openvino_fp32\", \"onnxruntime_fp32\"],\n",
        "             latency_sample_num=10)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The example output of `optimizer.optimize` is shown below.\n",
        "\n",
        "```bash\n",
        "==========================Optimization Results==========================\n",
        " -------------------------------- ---------------------- -------------- ----------------------\n",
        "|             method             |        status        | latency(ms)  |     metric value     |\n",
        " -------------------------------- ---------------------- -------------- ----------------------\n",
        "|            original            |      successful      |   108.209    |        0.994         |\n",
        "|         openvino_fp32          |      successful      |    30.325    |        0.994*        |\n",
        "|        onnxruntime_fp32        |      successful      |    31.313    |        0.994*        |\n",
        " -------------------------------- ---------------------- -------------- ----------------------\n",
        "* means we assume the metric value of the traced model does not change, so we don't recompute metric value to save time.\n",
        "Optimization cost 133.9s in total.\n",
        "```"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. In some cases, if you expect that some acceleration methods will not work for your model / not work well / run for too long / cause exceptions to the program, you could avoid running these methods by specifying `excludes` paramater:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.metrics import CategoricalAccuracy\n",
        "\n",
        "opt.optimize(ori_model,\n",
        "             x=calibration_set,\n",
        "             validation_data=validation_set,\n",
        "             metric=CategoricalAccuracy(),\n",
        "             direction=\"max\",\n",
        "             excludes=[\"int8\", \"onnxruntime_int8_integer\"],\n",
        "             latency_sample_num=10)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The example output of `optimizer.optimize` is shown below.\n",
        "\n",
        "```bash\n",
        "==========================Optimization Results==========================\n",
        " -------------------------------- ---------------------- -------------- ----------------------\n",
        "|             method             |        status        | latency(ms)  |     metric value     |\n",
        " -------------------------------- ---------------------- -------------- ----------------------\n",
        "|            original            |      successful      |   110.496    |        0.994         |\n",
        "|         openvino_fp32          |      successful      |    30.778    |        0.994*        |\n",
        "|         openvino_int8          |      successful      |    38.152    |        0.994         |\n",
        "|        onnxruntime_fp32        |      successful      |    23.143    |        0.994*        |\n",
        "|    onnxruntime_int8_qlinear    |      successful      |    12.721    |        0.963         |\n",
        " -------------------------------- ---------------------- -------------- ----------------------\n",
        "* means we assume the metric value of the traced model does not change, so we don't recompute metric value to save time.\n",
        "Optimization cost 323.0s in total.\n",
        "```"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Obtain specific model"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "You could call `get_best_model` method to obtain the best model under specific restrictions or without restrictions. Here we get the model with minimal latency when accuracy drop less than 5%."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.metrics import CategoricalAccuracy\n",
        "\n",
        "opt.optimize(ori_model,\n",
        "             x=calibration_set,\n",
        "             validation_data=validation_set,\n",
        "             metric=CategoricalAccuracy(),\n",
        "             direction=\"max\",\n",
        "             latency_sample_num=10)\n",
        "\n",
        "acc_model, option = opt.get_best_model(accuracy_criterion=0.05)\n",
        "print(\"When accuracy drop less than 5%, the model with minimal latency is: \", option)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "When accuracy drop less than 5%, the model with minimal latency is:  inc + onnxruntime + integer\n"
          ]
        }
      ],
      "execution_count": 5,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ðŸ“ **Note**\n",
        "> \n",
        ">  If you want to find the best model with `accuracy_criterion` paramter, make sure you have called `optimize` with validation data."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you just want to obtain a specific model although it doesn't have the minimal latency, you could call `get_model` method and specify `method_name`. Here we take `openvino_fp32` as an example:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "oepnvino_model = opt.get_model(method_name='openvino_fp32')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then you could use the obtained model for inference. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "for img, _ in tqdm(test_set):\n",
        "    acc_model(img)"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ðŸ“š **Related Readings**\n",
        "> \n",
        "> - [How to install BigDL-Nano](https://bigdl.readthedocs.io/en/latest/doc/Nano/Overview/nano.html#install)\n",
        "> - [How to install BigDL-Nano in Google Colab](https://bigdl.readthedocs.io/en/latest/doc/Nano/Howto/install_in_colab.html)"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10 (default, Jun  4 2021, 14:48:32) \n[GCC 7.5.0]"
    },
    "vscode": {
      "interpreter": {
        "hash": "d347a5dca25745bedb029e46e41f7d6c8c9b5181ecb97033e2e81a7538459254"
      }
    },
    "nteract": {
      "version": "0.28.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}