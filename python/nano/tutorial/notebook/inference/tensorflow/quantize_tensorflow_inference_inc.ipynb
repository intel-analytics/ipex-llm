{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[View the runnable example on GitHub](https://github.com/intel-analytics/BigDL/tree/main/python/nano/tutorial/notebook/inference/tensorflow/quantize_tensorflow_inference_inc.ipynb)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantize Tensorflow Model for Inference using Intel Neural Compressor"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "With Intel Neural Compressor (INC) as quantization engine, you can apply `InferenceOptimizer.quantize` API to realize post-training quantization on your Tensorflow Keras models, which takes only a few lines."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "To quantize your model with INC, the following dependencies need to be installed first:"
      ],
      "metadata": {
        "nbsphinx": "hidden"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for BigDL-Nano\n",
        "!pip install --pre --upgrade bigdl-nano[tensorflow,inference]\n",
        "!source bigdl-nano-init"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "nbsphinx": "hidden"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 📝 **Note**\n",
        "> \n",
        "> We recommend to run the commands above, especially `source bigdl-nano-init` before jupyter kernel is started, or some of the optimizations may not take effect."
      ],
      "metadata": {
        "nbsphinx": "hidden"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take an [EfficientNetB0 model](https://www.tensorflow.org/api_docs/python/tf/keras/applications/efficientnet/EfficientNetB0) pretrained on ImageNet dataset and finetuned on [Imagenette](https://www.tensorflow.org/datasets/catalog/imagenette) dataset for validation as an example (seen full definition of `prepare_dataset` and `create_model` in [runnable example](https://github.com/intel-analytics/BigDL/tree/main/python/nano/tutorial/notebook/inference/tensorflow/quantize_tensorflow_inference_inc.ipynb)):"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from bigdl.nano.tf.keras import Model\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        "\n",
        "def create_model(num_classes=10, img_size=224):\n",
        "    inputs = layers.Input(shape=(img_size, img_size, 3))\n",
        "    x = tf.cast(inputs, tf.float32)\n",
        "    backbone = EfficientNetB0(weights='imagenet', include_top=False)\n",
        "    backbone.trainable = False\n",
        "    x = backbone(x)\n",
        "    x = layers.GlobalAveragePooling2D('channels_last')(x)\n",
        "    x = layers.Dense(512, activation='relu')(x)\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
        "    return model"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "nbsphinx": "hidden"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "And we obtain our training and testing dataset as follows. "
      ],
      "metadata": {
        "nbsphinx": "hidden"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "def prepare_dataset(img_size=224, batch_size=512):\n",
        "    (ds_train, ds_test), ds_info = tfds.load(\n",
        "        'imagenette/320px-v2',\n",
        "        data_dir=\"./data/\",\n",
        "        split=['train', 'validation'],\n",
        "        with_info=True,\n",
        "        as_supervised=True\n",
        "    )\n",
        "\n",
        "    num_classes = ds_info.features['label'].num_classes\n",
        "\n",
        "    def preprocessing(input_img, label):\n",
        "        return tf.image.resize(input_img, (img_size, img_size)), tf.one_hot(label, num_classes)\n",
        "\n",
        "    AUTOTUNE = tf.data.AUTOTUNE\n",
        "    ds_train_batched = ds_train.shuffle(1000).map(preprocessing).batch(batch_size, drop_remainder=False).prefetch(AUTOTUNE)\n",
        "    ds_test = ds_test.map(preprocessing).batch(batch_size, drop_remainder=False).prefetch(AUTOTUNE)\n",
        "    calib_set = ds_train.map(preprocessing).prefetch(AUTOTUNE)\n",
        "\n",
        "    return ds_train_batched, ds_test, calib_set, ds_info "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nbsphinx": "hidden"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we finetune the model by following codes."
      ],
      "metadata": {
        "nbsphinx": "hidden"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set, test_set, calibration_set, ds_info = prepare_dataset()\n",
        "ori_model = create_model()\n",
        "ori_model.fit(train_set,\n",
        "          epochs=10,\n",
        "          steps_per_epoch=(ds_info.splits['train'].num_examples // 512 + 1),\n",
        "          )"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To enable quantization using INC for inference, you could simply **import BigDL-Nano** `InferenceOptimizer`**, and use** `InferenceOptimizer` **to quantize your TensorFlow model**:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from bigdl.nano.tf.keras import InferenceOptimizer\n",
        "\n",
        "q_model = InferenceOptimizer.quantize(ori_model, \n",
        "                                      x=calibration_set)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 📝 **Note**\n",
        "> \n",
        ">  `InferenceOptimizer` will by default quantize your TensorFlow models using int8 precision through **static** post-training quantization. Currently 'dynamic' approach is not supported yet. For this case, `x` (for calibration data) is required. To avoid data leak during calibration, it is suggested using training dataset or the subset of training set. \n",
        "> \n",
        "> Please refer to [API documentation](https://bigdl.readthedocs.io/en/latest/doc/PythonAPI/Nano/tensorflow.html) for more information on `InferenceOptimizer.quantize`."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "You could then do the normal inference steps with the quantized model:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.random.normal(shape=(2, 224, 224, 3))\n",
        "# use the optimized model here\n",
        "y_hat = q_model(x)\n",
        "predictions = tf.argmax(y_hat, axis=1)\n",
        "print(predictions)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 📚 **Related Readings**\n",
        "> \n",
        "> - [How to install BigDL-Nano](https://bigdl.readthedocs.io/en/latest/doc/Nano/Overview/nano.html#install)\n",
        "> - [How to install BigDL-Nano in Google Colab](https://bigdl.readthedocs.io/en/latest/doc/Nano/Howto/install_in_colab.html)"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.7.13 ('nano-pytorch': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "09344c7f3239fd422839751f876786d6b1a624c40f19af1b43cb2737f421c2b2"
      }
    },
    "nteract": {
      "version": "0.28.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}