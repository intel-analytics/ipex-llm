{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[View the runnable example on GitHub](https://github.com/intel-analytics/BigDL/tree/main/python/nano/tutorial/notebook/inference/tensorflow/quantize_tensorflow_inference_inc.ipynb)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantize Tensorflow Model for Inference using Intel Neural Compressor"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "With Intel Neural Compressor (INC) as quantization engine, you can apply `InferenceOptimizer.quantize` API to realize post-training quantization on your Tensorflow Keras models, which takes only a few lines."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "To quantize your model with INC, the following dependencies need to be installed first:"
      ],
      "metadata": {
        "nbsphinx": "hidden"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for BigDL-Nano\n",
        "!pip install --pre --upgrade bigdl-nano[tensorflow,inference]\n",
        "!source bigdl-nano-init"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "nbsphinx": "hidden"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ðŸ“ **Note**\n",
        "> \n",
        "> We recommend to run the commands above, especially `source bigdl-nano-init` before jupyter kernel is started, or some of the optimizations may not take effect."
      ],
      "metadata": {
        "nbsphinx": "hidden"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take an [EfficientNetB0 model](https://www.tensorflow.org/api_docs/python/tf/keras/applications/efficientnet/EfficientNetB0) pretrained on ImageNet dataset as an example:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "\n",
        "model = EfficientNetB0(weights='imagenet')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "And we obtain our training and testing dataset as follows. "
      ],
      "metadata": {
        "nbsphinx": "hidden"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "batch_size = 128    \n",
        "img_size = 224\n",
        "  \n",
        "def prepare_dataset():\n",
        "  \n",
        "    (ds_train, ds_test), ds_info = tfds.load(\n",
        "        \"imagenet2012\",\n",
        "        data_dir=\"./data/\",\n",
        "        split=['train', 'test'],\n",
        "        with_info=True,\n",
        "        as_supervised=True\n",
        "    )\n",
        "\n",
        "    num_classes = ds_info.features['label'].num_classes\n",
        "\n",
        "    def preprocessing(img, label):\n",
        "        return tf.image.resize(img, (img_size, img_size)), tf.one_hot(label, num_classes)\n",
        "\n",
        "    AUTOTUNE = tf.data.AUTOTUNE\n",
        "    ds_train = ds_train.shuffle(1000).map(preprocessing).batch(batch_size, drop_remainder=False).prefetch(AUTOTUNE)\n",
        "    ds_test = ds_test.map(preprocessing).batch(batch_size, drop_remainder=False).prefetch(AUTOTUNE)\n",
        "\n",
        "    for img, _ in tqdm(ds_train):\n",
        "        calib_set = img\n",
        "        break\n",
        "        \n",
        "    return ds_train, ds_test, calib_set "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nbsphinx": "hidden"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ðŸ“ **Note**\n",
        "> \n",
        ">  `tensorflow_dataset` requires downloading the source data manually into `download_config.manual_dir` (defaults to ~/tensorflow_datasets/downloads/manual/): manual_dir should contain two files: ILSVRC2012_img_train.tar and ILSVRC2012_img_val.tar.\n",
        "> \n",
        ">  Please refer to [Tensorflow documentation](https://www.tensorflow.org/datasets/catalog/imagenet2012) for more information. "
      ],
      "metadata": {
        "nbsphinx": "hidden"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To enable quantization using INC for inference, you could simply **import BigDL-Nano** `InferenceOptimizer`**, and use** `InferenceOptimizer` **to quantize your TensorFlow model**:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from bigdl.nano.tf.keras import InferenceOptimizer\n",
        "\n",
        "train_set, test_set, calibration_set = prepare_dataset()\n",
        "q_model = InferenceOptimizer.quantize(model, \n",
        "                                      x=calibration_set)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ðŸ“ **Note**\n",
        "> \n",
        ">  `InferenceOptimizer` will by default quantize your TensorFlow models using int8 precision through **static** post-training quantization. Currently 'dynamic' approach is not supported yet. For this case, `x` (for calibration data) is required. To avoid data leak during calibration, it is suggested using training dataset or the subset of training set. \n",
        "> \n",
        "> Please refer to [API documentation](https://bigdl.readthedocs.io/en/latest/doc/PythonAPI/Nano/tensorflow.html) for more information on `InferenceOptimizer.quantize`."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "You could then do the normal inference steps with the quantized model:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.random.normal(shape=(2, 224, 224, 3))\n",
        "# use the optimized model here\n",
        "y_hat = ort_model(x)\n",
        "predictions = tf.argmax(y_hat, axis=1)\n",
        "print(predictions)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ðŸ“š **Related Readings**\n",
        "> \n",
        "> - [How to install BigDL-Nano](https://bigdl.readthedocs.io/en/latest/doc/Nano/Overview/nano.html#install)\n",
        "> - [How to install BigDL-Nano in Google Colab](https://bigdl.readthedocs.io/en/latest/doc/Nano/Howto/install_in_colab.html)"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.7.13 ('nano-pytorch': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "09344c7f3239fd422839751f876786d6b1a624c40f19af1b43cb2737f421c2b2"
      }
    },
    "nteract": {
      "version": "0.28.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}