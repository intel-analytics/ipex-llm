{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[View the runnable example on GitHub](https://github.com/intel-analytics/BigDL/tree/main/python/nano/tutorial/notebook/inference/pytorch/accelerate_pytorch_inference_jit_ipex.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Accelerate PyTorch Inference in Precision Bfloat16\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> ðŸ“ **Note**\n",
        ">\n",
        "> * `bf16`: All inference optimizations in bfloat16 precision should use the `InferenceOptimizer.quantize(..., precision='bf16')`.\n",
        "> * `bf16 + ipex`: You can use `InferenceOptimizer.quantize(..., precision='bf16', use_ipex=True)` API to enable the ipex acceleration for PyTorch inference.\n",
        "> * `bf16 + jit`: You can use `InferenceOptimizer.quantize(..., precision='bf16', accelerator=\"jit\")` API to enable the jit acceleration for PyTorch inference.\n",
        "> * `bf16 + channels_last`: You can use `InferenceOptimizer.quantize(..., precision='bf16', channels_last=True)` to use channels last memory format for PyTorch inference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To accelerate the model in bf16 precision, the following dependencies need to be installed firstï¼š"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for BigDL-Nano\n",
        "!pip install --pre --upgrade bigdl-nano[pytorch]  # install the nightly-bulit version\n",
        "# !source bigdl-nano-init\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> ðŸ“ **Note**\n",
        ">\n",
        "> We recommend to run the commands above, especially `source bigdl-nano-init` before jupyter kernel is started, or some of the optimizations may not take effect."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's take an [ResNet-18 model](https://pytorch.org/vision/main/models/generated/torchvision.models.resnet18.html) pretrained on ImageNet dataset as an example. First, we load the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision.models import resnet18\n",
        "\n",
        "model_ft = resnet18(pretrained=True)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we set it in evaluation mode:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_ft.eval()\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Accelerate the model in bf16 precision, we need import `InferenceOptimizer` firstly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from bigdl.nano.pytorch import InferenceOptimizer\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inference with bfloat16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = torch.rand(2, 3, 224, 224)\n",
        "bf16_model = InferenceOptimizer.quantize(model_ft,\n",
        "                                            precision='bf16',\n",
        "                                            input_sample=torch.rand(1, 3, 224, 224))\n",
        "with InferenceOptimizer.get_context(bf16_model):\n",
        "    y_hat = bf16_model(x)\n",
        "    predictions = y_hat.argmax(dim=1)\n",
        "    print(predictions)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Accelerate inference using IPEX optimizer in bf16 precision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ipex_model = InferenceOptimizer.quantize(model_ft,\n",
        "                                         precision='bf16',\n",
        "                                         use_ipex=True,\n",
        "                                         input_sample=torch.rand(1, 3, 224, 224))\n",
        "with InferenceOptimizer.get_context(ipex_model):\n",
        "    y_hat = ipex_model(x)\n",
        "    predictions = y_hat.argmax(dim=1)\n",
        "    print(predictions)\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Accelerate inference using JIT in bf16 precision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "jit_model = InferenceOptimizer.quantize(model_ft,\n",
        "                                        precision='bf16',\n",
        "                                        accelerator=\"jit\",\n",
        "                                        input_sample=torch.rand(1, 3, 224, 224))\n",
        "with InferenceOptimizer.get_context(jit_model):\n",
        "    y_hat = jit_model(x)\n",
        "    predictions = y_hat.argmax(dim=1)\n",
        "    print(predictions)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Accelerate inference using channels last memory format in bf16 precision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "jit_model = InferenceOptimizer.quantize(model_ft,\n",
        "                                        precision='bf16',\n",
        "                                        channels_last=True,\n",
        "                                        input_sample=torch.rand(1, 3, 224, 224))\n",
        "with InferenceOptimizer.get_context(jit_model):\n",
        "    y_hat = jit_model(x)\n",
        "    predictions = y_hat.argmax(dim=1)\n",
        "    print(predictions)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Accelereate inference using combined method\n",
        "You can use any of the methods mentioned above to combine each other to try to acclerate your model. However, the effect is not like stacking buffs.\n",
        "It is not that the more methods you use, the better. You should try many times to find the best combination of methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# bf16 + IPEX + JIT + channels_last\n",
        "jit_ipex_model = InferenceOptimizer.quantize(model_ft,\n",
        "                                             precision='bf16',\n",
        "                                             accelerator=\"jit\",\n",
        "                                             use_ipex=True,\n",
        "                                             channels_last=True,\n",
        "                                             input_sample=torch.rand(1, 3, 224, 224))\n",
        "with InferenceOptimizer.get_context(jit_ipex_model):\n",
        "    y_hat = jit_ipex_model(x)\n",
        "    predictions = y_hat.argmax(dim=1)\n",
        "    print(predictions)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> ðŸ“ **Note**\n",
        ">\n",
        "> `input_sample` is the parameter for OpenVINO accelerator to know the **shape** of the model input. So both the batch size and the specific values are not important to `input_sample`. If we want our test dataset to consist of images with $224 \\times 224$ pixels, we could use `torch.rand(1, 3, 224, 224)` for `input_sample` here.\n",
        ">\n",
        "> Please refer to [API documentation](https://bigdl.readthedocs.io/en/latest/doc/PythonAPI/Nano/pytorch.html#bigdl.nano.pytorch.InferenceOptimizer.quantize) for more information on `InferenceOptimizer.quantize`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> ðŸ“š **Related Readings**\n",
        ">\n",
        "> - [How to install BigDL-Nano](https://bigdl.readthedocs.io/en/latest/doc/Nano/Overview/nano.html#install)\n",
        "> - [How to install BigDL-Nano in Google Colab](https://bigdl.readthedocs.io/en/latest/doc/Nano/Howto/install_in_colab.html)"
      ]
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": 3
    },
    "orig_nbformat": 4
  }
}