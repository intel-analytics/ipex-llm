{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[View the runnable example on GitHub](https://github.com/intel-analytics/BigDL/tree/main/python/nano/tutorial/notebook/inference/pytorch/accelerate_pytorch_inference_jit_ipex.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Accelerate PyTorch Inference using JIT/IPEX\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> ðŸ“ **Note**\n",
        ">\n",
        "> * `jit`: You can use `InferenceOptimizer.trace(..., accelerator=\"jit\")` API to enable the jit acceleration for PyTorch inference.\n",
        "> * `ipex`: You can use`InferenceOptimizer.trace(...,use_ipex=True)` API to enable the ipex acceleration for PyTorch inference. It only takes a few lines.\n",
        "> * `jit + ipex`: It is recommended to use JIT and IPEX together. You can user `InferenceOptimizer.trace(..., acclerator=\"jit\", use_ipex=True`) to enable both for PyTorch inference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To apply JIT/IPEX acceleration, the following dependencies need to be installed firstï¼š"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for BigDL-Nano\n",
        "!pip install --pre --upgrade bigdl-nano[pytorch]  # install the nightly-bulit version\n",
        "# !source bigdl-nano-init"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> ðŸ“ **Note**\n",
        ">\n",
        "> We recommend to run the commands above, especially `source bigdl-nano-init` before jupyter kernel is started, or some of the optimizations may not take effect."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's take an [ResNet-18 model](https://pytorch.org/vision/main/models/generated/torchvision.models.resnet18.html) pretrained on ImageNet dataset as an example. First, we load the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision.models import resnet18\n",
        "\n",
        "model_ft = resnet18(pretrained=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we set it in evaluation mode:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_ft.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Accelerate Inference Using JIT/IPEX/JIT+IPEX, we need import `InferenceOptimizer` firstly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from bigdl.nano.pytorch import InferenceOptimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Accelerate Inference Using JIT Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "jit_model = InferenceOptimizer.trace(model_ft,\n",
        "                                     accelerator=\"jit\",\n",
        "                                     input_sample=torch.rand(1, 3, 224, 224))\n",
        "with InferenceOptimizer.get_context(jit_model):\n",
        "    y_hat = jit_model(x)\n",
        "    predictions = y_hat.argmax(dim=1)\n",
        "    print(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Accelerate Inference Using IPEX Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ipex_model = InferenceOptimizer.trace(model_ft,\n",
        "                                      use_ipex=True)\n",
        "with InferenceOptimizer.get_context(ipex_model):\n",
        "    y_hat = ipex_model(x)\n",
        "    predictions = y_hat.argmax(dim=1)\n",
        "    print(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Accelerate Inference Using IPEX + JIT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "jit_ipex_model = InferenceOptimizer.trace(model_ft,\n",
        "                                          accelerator=\"jit\",\n",
        "                                          use_ipex=True,\n",
        "                                          input_sample=torch.rand(1, 3, 224, 224))\n",
        "with InferenceOptimizer.get_context(jit_ipex_model):\n",
        "    y_hat = jit_ipex_model(x)\n",
        "    predictions = y_hat.argmax(dim=1)\n",
        "    print(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> ðŸ“ **Note**\n",
        ">\n",
        "> `input_sample` is the parameter for OpenVINO accelerator to know the **shape** of the model input. So both the batch size and the specific values are not important to `input_sample`. If we want our test dataset to consist of images with $224 \\times 224$ pixels, we could use `torch.rand(1, 3, 224, 224)` for `input_sample` here.\n",
        ">\n",
        "> Please refer to [API documentation](https://bigdl.readthedocs.io/en/latest/doc/PythonAPI/Nano/pytorch.html#bigdl.nano.pytorch.InferenceOptimizer.trace) for more information on `InferenceOptimizer.trace`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> ðŸ“š **Related Readings**\n",
        ">\n",
        "> - [How to install BigDL-Nano](https://bigdl.readthedocs.io/en/latest/doc/Nano/Overview/nano.html#install)\n",
        "> - [How to install BigDL-Nano in Google Colab](https://bigdl.readthedocs.io/en/latest/doc/Nano/Howto/install_in_colab.html)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.7.10 ('ruonan_nano')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "d347a5dca25745bedb029e46e41f7d6c8c9b5181ecb97033e2e81a7538459254"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
