{"cells":[{"cell_type":"markdown","metadata":{},"source":["  [View the runnable example on GitHub](https://github.com/intel-analytics/BigDL/tree/main/python/nano/tutorial/notebook/inference/pytorch/pytorch_save_and_load_jit.ipynb)"]},{"cell_type":"markdown","metadata":{},"source":["  # Save and Load Optimized JIT Model\n","  This example illustrates how to save and load a model accelerated by JIT.\n","  In this example, we use a ResNet18 model pretrained. Then, by calling `InferenceOptimizer.trace(..., user_ipex=True)`, we can obtain a model accelarated by JIT method. By calling `InferenceOptimizer.save(model_name, path)` , we could save the model to a folder. By calling `InferenceOptimizer.load(path)`, we could load the model from a folder."]},{"cell_type":"markdown","metadata":{},"source":["  To inference using Bigdl-nano InferenceOptimizer, the following packages need to be installed first. We recommend you to use [Miniconda](https://docs.conda.io/en/latest/miniconda.html) to prepare the environment and install the following packages in a conda environment.\n","\n","  You can create a conda environment by executing:\n","\n","  ```\n","  # \"nano\" is conda environment name, you can use any name you like.\n","  conda create -n nano python=3.7 setuptools=58.0.4\n","  conda activate nano\n","  ```\n"," > ðŸ“ **Note**\n"," >\n"," > During your installation, there may be some warnings or errors about version, just ignore them."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Necessary packages for inference accelaration\n","!pip install --pre --upgrade bigdl-nano[pytorch]"]},{"cell_type":"markdown","metadata":{},"source":["  First, prepare model. We need load the pretrained ResNet18 model."]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/cpx/anaconda3/envs/junwang-resnext-oob/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","/home/cpx/anaconda3/envs/junwang-resnext-oob/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n","/home/cpx/anaconda3/envs/junwang-resnext-oob/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]}],"source":["import torch\n","from torchvision.models import resnet18\n","\n","model_ft = resnet18(pretrained=True)"]},{"cell_type":"markdown","metadata":{},"source":[" Accelerated Inference Using JIT / JIT+IPEX"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-11-17 14:14:27,390 - torch.distributed.nn.jit.instantiator - INFO - Created a temporary directory at /tmp/tmpvkwjqs4f\n","2022-11-17 14:14:27,391 - torch.distributed.nn.jit.instantiator - INFO - Writing /tmp/tmpvkwjqs4f/_remote_module_non_scriptable.py\n"]}],"source":["from bigdl.nano.pytorch import InferenceOptimizer\n","jit_model = InferenceOptimizer.trace(model_ft,\n","                                        accelerator=\"jit\",\n","                                        input_sample=torch.rand(1, 3, 224, 224))"]},{"cell_type":"markdown","metadata":{},"source":[" Save Optimized JIT Model\n"," The saved model files will be saved at \"./optimized_model_jit\" directory\n"," There are 2 files in optimized_model_jit, users only need to take \"ckpt.pth\" file for further usage:\n"," * nano_model_meta.yml: meta information of the saved model checkpoint\n"," * ckpt.pth: JIT model checkpoint for general use, describes model structure"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["InferenceOptimizer.save(jit_model, \"./optimized_model_jit\")"]},{"cell_type":"markdown","metadata":{},"source":[" Load the Optimized Model"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["loaded_model = InferenceOptimizer.load(\"./optimized_model_jit\")"]},{"cell_type":"markdown","metadata":{},"source":[" Inference with the Loaded Model"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([111, 111])\n"]}],"source":["x = torch.rand(2, 3, 224, 224)\n","y_hat = loaded_model(x)\n","predictions = y_hat.argmax(dim=1)\n","print(predictions)"]},{"cell_type":"markdown","metadata":{},"source":["\n","  > ðŸ“š **Related Readings**\n","  >\n","  > - [How to install BigDL-Nano](https://bigdl.readthedocs.io/en/latest/doc/Nano/Overview/nano.html#install)\n","  > - [How to install BigDL-Nano in Google Colab](https://bigdl.readthedocs.io/en/latest/doc/Nano/Howto/install_in_colab.html)"]}],"metadata":{"kernelspec":{"display_name":"Python 3.7.13 ('junwang-resnext-oob')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.13"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"2c2c667a59d63f4d9cf9e9a8f7eff73ad81424da777ad3c4a3346b0ce2b012b2"}}},"nbformat":4,"nbformat_minor":2}
