{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    .rst-content blockquote {\n",
    "        margin-left: 0px;\n",
    "    }\n",
    "\n",
    "    blockquote > div {\n",
    "        margin: 1.5625em auto;\n",
    "        padding: 20px 15px 1px;\n",
    "        border-left: 0.2rem solid rgb(59, 136, 219);  \n",
    "        border-radius: 0.2rem;\n",
    "        box-shadow: 0 0.2rem 0.5rem rgb(0 0 0 / 5%), 0 0 0.0625rem rgb(0 0 0 / 10%);\n",
    "    }\n",
    "</style>\n",
    "\n",
    "[View the runnable example on GitHub](https://github.com/intel-analytics/BigDL/tree/main/python/nano/tutorial/notebook/inference/pytorch/inference_optimizer_optimize.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Acceleration Method with the Minimum Inference Latency using InferenceOptimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example illustrates how to apply InferenceOptimizer to quickly find acceleration method with the minimum inference latency under specific restrictions or without restrictions for a trained model. \n",
    "In this example, we first train ResNet18 model on the [cats and dogs dataset](https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip). Then, by calling `optimize()`, we can obtain all available accelaration combinations provided by BigDL-Nano for inference. By calling `get_best_model()` , we could get the best model under specific restrictions or without restrictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To inference using Bigdl-nano InferenceOptimizer, the following packages need to be installed first. We recommend you to use [Miniconda](https://docs.conda.io/en/latest/miniconda.html) to prepare the environment and install the following packages in a conda environment. \n",
    "\n",
    "You can create a conda environment by executing:\n",
    "\n",
    "```\n",
    "conda create -n nano python=3.7 setuptools=58.0.4  # \"nano\" is conda environment name, you can use any name you like.\n",
    "conda activate nano\n",
    "```\n",
    "\n",
    "**Note**: during your installation, there may be some warnings or errors about version, just ignore them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --pre --upgrade bigdl-nano[pytorch]\n",
    "\n",
    "# bf16 is available only on torch1.12\n",
    "!pip install torch==1.12.0 torchvision --extra-index-url https://download.pytorch.org/whl/cpu \n",
    "# Necessary packages for inference accelaration\n",
    "!pip install --upgrade intel-extension-for-pytorch\n",
    "!pip install onnx==1.12.0 onnxruntime==1.12.1 onnxruntime-extensions\n",
    "!pip install openvino-dev\n",
    "!pip install neural-compressor==1.12\n",
    "!pip install --upgrade numpy==1.21.6\n",
    "# Necessary packages for running this notebook\n",
    "!pip install ipykernal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then initialize environment variables with script `bigdl-nano-init` installed with bigdl-nano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!source bigdl-nano-init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, prepare model and dataset. We use a pretrained ResNet18 model and train the model on [cats and dogs dataset](https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip) in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from torchmetrics import Accuracy\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.datasets.utils import download_and_extract_archive\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from bigdl.nano.pytorch import Trainer\n",
    "\n",
    "def accuracy(pred, target):\n",
    "    pred = torch.sigmoid(pred)\n",
    "    return Accuracy()(pred, target)\n",
    "\n",
    "def prepare_model_and_dataset(model_ft, val_size):\n",
    "    DATA_URL = \"https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\"\n",
    "\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    if not Path(\"data\").exists():\n",
    "        # download dataset\n",
    "        download_and_extract_archive(url=DATA_URL, download_root=\"data\", remove_finished=True)\n",
    "\n",
    "    data_path = Path(\"data/cats_and_dogs_filtered\")\n",
    "    train_dataset = ImageFolder(data_path.joinpath(\"train\"), transform=train_transform)\n",
    "    val_dataset = ImageFolder(data_path.joinpath(\"validation\"), transform=val_transform)\n",
    "\n",
    "    indices = torch.randperm(len(val_dataset))\n",
    "    val_dataset = Subset(val_dataset, indices=indices[:val_size])\n",
    "\n",
    "    train_dataloader = DataLoader(dataset=train_dataset, batch_size=8, shuffle=True)\n",
    "    val_dataloader = DataLoader(dataset=val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "    num_ftrs = model_ft.fc.in_features\n",
    "    \n",
    "    model_ft.fc = torch.nn.Linear(num_ftrs, 2)\n",
    "    loss_ft = torch.nn.CrossEntropyLoss()\n",
    "    optimizer_ft = torch.optim.Adam(model_ft.parameters(), lr=1e-3)\n",
    "\n",
    "    # compile model\n",
    "    model = Trainer.compile(model_ft, loss=loss_ft, optimizer=optimizer_ft, metrics=[accuracy])\n",
    "    trainer = Trainer(max_epochs=1)\n",
    "    trainer.fit(model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)\n",
    "    \n",
    "    return model, train_dataset, val_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18\n",
    "\n",
    "model = resnet18(pretrained=True)\n",
    "_, train_dataset, val_dataset = prepare_model_and_dataset(model, val_size=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;_The full definition of function_ `prepare_model_and_dataset` _could be found in the_ [runnable example](https://github.com/intel-analytics/BigDL/tree/main/python/nano/tutorial/notebook/inference/pytorch/inference_optimizer_optimize.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find acceleration method with the minimum inference latency, you could import `InferenceOptimizer` and call `optimize` method. The `optimize` method will run all possible acceleration combinations and output the result, it will take about 2 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigdl.nano.pytorch import InferenceOptimizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define metric for accuracy calculation\n",
    "def accuracy(pred, target):\n",
    "    pred = torch.sigmoid(pred)\n",
    "    return Accuracy()(pred, target)\n",
    "\n",
    "optimizer = InferenceOptimizer()\n",
    "\n",
    "# To obtain the latency of single sample, set batch_size=1\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1)\n",
    "val_dataloader = DataLoader(val_dataset)\n",
    "\n",
    "optimizer.optimize(model=model,\n",
    "                   training_data=train_dataloader,\n",
    "                   validation_data=val_dataloader,\n",
    "                   metric=accuracy,\n",
    "                   direction=\"max\",\n",
    "                   thread_num=1,\n",
    "                   latency_sample_num=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example output of `optimizer.optimize` is shown below.\n",
    "\n",
    "```\n",
    "==========================Optimization Results==========================\n",
    " -------------------------------- ---------------------- -------------- ----------------------\n",
    "|             method             |        status        | latency(ms)  |       accuracy       |\n",
    " -------------------------------- ---------------------- -------------- ----------------------\n",
    "|            original            |      successful      |    41.304    |         0.86         |\n",
    "|           fp32_ipex            |      successful      |    38.624    |    not recomputed    |\n",
    "|              bf16              |   lack dependency    |     None     |         None         |\n",
    "|           bf16_ipex            |   lack dependency    |     None     |         None         |\n",
    "|              int8              |      successful      |    23.108    |        0.852         |\n",
    "|            jit_fp32            |    early stopped     |    75.324    |         None         |\n",
    "|         jit_fp32_ipex          |      successful      |    65.829    |    not recomputed    |\n",
    "|  jit_fp32_ipex_channels_last   |    early stopped     |    90.795    |         None         |\n",
    "|         openvino_fp32          |      successful      |    40.322    |    not recomputed    |\n",
    "|         openvino_int8          |      successful      |    3.871     |        0.834         |\n",
    "|        onnxruntime_fp32        |      successful      |    30.08     |    not recomputed    |\n",
    "|    onnxruntime_int8_qlinear    |      successful      |    18.662    |        0.846         |\n",
    "|    onnxruntime_int8_integer    |   fail to convert    |     None     |         None         |\n",
    " -------------------------------- ---------------------- -------------- ----------------------\n",
    "Optimization cost 74.2s in total.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ“ **Note**\n",
    "> \n",
    "> When specifying `training_data` parameter, make sure to set batch size of the training data to the same batch size you may want to use in real deploy environment, as the batch size may impact on latency.\n",
    ">\n",
    "> For more information, please refer to the [API Documentation](https://bigdl.readthedocs.io/en/latest/doc/PythonAPI/Nano/pytorch.html#bigdl.nano.pytorch.InferenceOptimizer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could call `get_best_model` method to obtain the best model under specific restrictions or without restrictions. Here we get the model with minimal latency when accuracy drop less than 5%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When accuracy drop less than 5%, the model with minimal latency is:  openvino + int8 \n"
     ]
    }
   ],
   "source": [
    "acc_model, option = optimizer.get_best_model(accuracy_criterion=0.05)\n",
    "print(\"When accuracy drop less than 5%, the model with minimal latency is: \", option)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you could use the best model for inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(iter(train_dataloader))[0]\n",
    "output = acc_model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To export the best model, you could simply call `save` method and pass the path to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"./best_model\"\n",
    "InferenceOptimizer.save(acc_model, save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model files will be saved at `./best_model` directory. For each type in the `option` of best model, you only need to take the following files for further usage.\n",
    "\n",
    "- **OpenVINO**\n",
    "    \n",
    "    `ov_saved_model.bin`: contains the weights and biases binary data of model\n",
    "    \n",
    "    `ov_saved_model.xml`: model checkpoint for general use, describes model structure\n",
    "\n",
    "- **onnxruntime**\n",
    "\n",
    "    `onnx_saved_model.onnx`: represents model checkpoint for general use, describes model structure\n",
    "    \n",
    "- **int8**\n",
    "\n",
    "    `best_model.pt`: represents model optimized by IntelÂ® Neural Compressor\n",
    "\n",
    "- **ipex | channel_last | jit**\n",
    "    \n",
    "    `ckpt.pt`: if `jit` in option, it stores model optimized using just-in-time compilation, otherwise, it stores original model weight by `torch.save(model.state_dict())`.\n",
    "\n",
    "- **Others**\n",
    "    \n",
    "    `saved_weight.pt`: saved by `torch.save(model.state_dict())`.\n",
    "    \n",
    "    if `bf16` in option, the model weights obtained are bf16 dtype, otherwise, the model weights obtained are fp32 dtype"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('nano')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "05c6b89ab4c9dc24f84cc5c8dcf96192b5e96984290c1f42e71f9e350c660fc9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
