#  ğŸ’« IntelÂ® LLM Library for PyTorch* 
<p>
  < <a href='./README.md'>English</a> | <a href='./README.zh-CN.md'>ä¸­æ–‡</a> | <b>æ—¥æœ¬èª</b> >
</p>

**`IPEX-LLM`**ã¯ã€Intel ***CPU***ã€***GPU***ï¼ˆä¾‹ï¼šiGPUã‚’æ­è¼‰ã—ãŸãƒ­ãƒ¼ã‚«ãƒ«PCã€Arcã€Flexã€Maxãªã©ã®ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ¼ãƒˆGPUï¼‰ãŠã‚ˆã³***NPU***[^1]ç”¨ã®LLMåŠ é€Ÿãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚
> [!NOTE]
> - *ã“ã‚Œã¯ã€**`llama.cpp`**ã€**`transformers`**ã€**`bitsandbytes`**ã€**`vLLM`**ã€**`qlora`**ã€**`AutoGPTQ`**ã€**`AutoAWQ`**ãªã©ã®å„ªã‚ŒãŸä½œæ¥­ã«åŸºã¥ã„ã¦æ§‹ç¯‰ã•ã‚Œã¦ã„ã¾ã™ã€‚*
> - *ã“ã‚Œã¯ã€[llama.cpp](docs/mddocs/Quickstart/llama_cpp_quickstart.md)ã€[Ollama](docs/mddocs/Quickstart/ollama_quickstart.md)ã€[HuggingFace transformers](python/llm/example/GPU/HuggingFace)ã€[LangChain](python/llm/example/GPU/LangChain)ã€[LlamaIndex](python/llm/example/GPU/LlamaIndex)ã€[vLLM](docs/mddocs/Quickstart/vLLM_quickstart.md)ã€[Text-Generation-WebUI](docs/mddocs/Quickstart/webui_quickstart.md)ã€[DeepSpeed-AutoTP](python/llm/example/GPU/Deepspeed-AutoTP)ã€[FastChat](docs/mddocs/Quickstart/fastchat_quickstart.md)ã€[Axolotl](docs/mddocs/Quickstart/axolotl_quickstart.md)ã€[HuggingFace PEFT](python/llm/example/GPU/LLM-Finetuning)ã€[HuggingFace TRL](python/llm/example/GPU/LLM-Finetuning/DPO)ã€[AutoGen](python/llm/example/CPU/Applications/autogen)ã€[ModeScope](python/llm/example/GPU/ModelScope-Models)ã¨ã‚·ãƒ¼ãƒ ãƒ¬ã‚¹ã«çµ±åˆã•ã‚Œã¦ã„ã¾ã™ã€‚*
> - ***70ä»¥ä¸Šã®ãƒ¢ãƒ‡ãƒ«**ãŒ`ipex-llm`ã§æœ€é©åŒ–/æ¤œè¨¼ã•ã‚Œã¦ãŠã‚Šï¼ˆä¾‹ï¼šLlamaã€Phiã€Mistralã€Mixtralã€Whisperã€Qwenã€MiniCPMã€Qwen-VLã€MiniCPM-Vãªã©ï¼‰ã€æœ€å…ˆç«¯ã®**LLMæœ€é©åŒ–**ã€**XPUåŠ é€Ÿ**ã€ãŠã‚ˆã³**ä½ãƒ“ãƒƒãƒˆï¼ˆFP8/FP6/FP4/INT4ï¼‰ã‚µãƒãƒ¼ãƒˆ**ã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚å®Œå…¨ãªãƒªã‚¹ãƒˆã¯[ã“ã¡ã‚‰](#verified-models)ã‚’ã”è¦§ãã ã•ã„ã€‚*

## æœ€æ–°ã®æ›´æ–° ğŸ”¥ 
- [2024/07] Microsoftã®**GraphRAG**ã‚’Intel GPUã§å®Ÿè¡Œã™ã‚‹ã‚µãƒãƒ¼ãƒˆã‚’è¿½åŠ ã—ã¾ã—ãŸã€‚ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆã‚¬ã‚¤ãƒ‰ã¯[ã“ã¡ã‚‰](docs/mddocs/Quickstart/graphrag_quickstart.md)ã‚’ã”è¦§ãã ã•ã„ã€‚
- [2024/07] å¤§è¦æ¨¡ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã®ã‚µãƒãƒ¼ãƒˆã‚’å¤§å¹…ã«å¼·åŒ–ã—ã¾ã—ãŸã€‚è©³ç´°ã¯[ã“ã¡ã‚‰](python/llm/example/GPU/HuggingFace/Multimodal)ã‚’ã”è¦§ãã ã•ã„ã€‚
- [2024/07] Intel [GPU](python/llm/example/GPU/HuggingFace/More-Data-Types)ã§**FP6**ã®ã‚µãƒãƒ¼ãƒˆã‚’è¿½åŠ ã—ã¾ã—ãŸã€‚
- [2024/06] Intel Core Ultraãƒ—ãƒ­ã‚»ãƒƒã‚µã®**NPU**ã‚µãƒãƒ¼ãƒˆã‚’å®Ÿé¨“çš„ã«è¿½åŠ ã—ã¾ã—ãŸã€‚è©³ç´°ã¯[ã“ã¡ã‚‰](python/llm/example/NPU/HF-Transformers-AutoModels)ã‚’ã”è¦§ãã ã•ã„ã€‚
- [2024/06] **ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä¸¦åˆ—**[æ¨è«–](python/llm/example/GPU/Pipeline-Parallel-Inference)ã®ã‚µãƒãƒ¼ãƒˆã‚’å¤§å¹…ã«å¼·åŒ–ã—ã¾ã—ãŸã€‚ã“ã‚Œã«ã‚ˆã‚Šã€2ã¤ä»¥ä¸Šã®Intel GPUï¼ˆä¾‹ï¼šArcï¼‰ã‚’ä½¿ç”¨ã—ã¦å¤§è¦æ¨¡ãªLLMã‚’å®Ÿè¡Œã™ã‚‹ã“ã¨ãŒå®¹æ˜“ã«ãªã‚Šã¾ã™ã€‚
- [2024/06] Intel [GPU](docs/mddocs/Quickstart/ragflow_quickstart.md)ã§**RAGFlow**ã‚’å®Ÿè¡Œã™ã‚‹ã‚µãƒãƒ¼ãƒˆã‚’è¿½åŠ ã—ã¾ã—ãŸã€‚
- [2024/05] **Axolotl**ã‚’ä½¿ç”¨ã—ã¦Intel GPUã§LLMã®å¾®èª¿æ•´ã‚’è¡Œã†ã‚µãƒãƒ¼ãƒˆã‚’è¿½åŠ ã—ã¾ã—ãŸã€‚ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆã‚¬ã‚¤ãƒ‰ã¯[ã“ã¡ã‚‰](docs/mddocs/Quickstart/axolotl_quickstart.md)ã‚’ã”è¦§ãã ã•ã„ã€‚

<details><summary>ã•ã‚‰ã«å¤šãã®æ›´æ–°</summary>
<br/>
 
- [2024/05] **Docker** [images](#docker)ã‚’ä½¿ç”¨ã—ã¦ã€`ipex-llm`ã®æ¨è«–ã€ã‚µãƒ¼ãƒ“ã‚¹ã€å¾®èª¿æ•´ã‚’ç°¡å˜ã«å®Ÿè¡Œã§ãã¾ã™ã€‚
- [2024/05] Windowsã§`ipex-llm`ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ãŸã‚ã®"*[one command](docs/mddocs/Quickstart/install_windows_gpu.md#install-ipex-llm)*"ã‚’è¿½åŠ ã—ã¾ã—ãŸã€‚
- [2024/04] `ipex-llm`ã‚’ä½¿ç”¨ã—ã¦Intel GPUã§**Open WebUI**ã‚’å®Ÿè¡Œã™ã‚‹ã‚µãƒãƒ¼ãƒˆã‚’è¿½åŠ ã—ã¾ã—ãŸã€‚ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆã‚¬ã‚¤ãƒ‰ã¯[ã“ã¡ã‚‰](docs/mddocs/Quickstart/open_webui_with_ollama_quickstart.md)ã‚’ã”è¦§ãã ã•ã„ã€‚
- [2024/04] `ipex-llm`ã‚’ä½¿ç”¨ã—ã¦Intel GPUã§**Llama 3**ã‚’å®Ÿè¡Œã™ã‚‹ã‚µãƒãƒ¼ãƒˆã‚’è¿½åŠ ã—ã¾ã—ãŸã€‚ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆã‚¬ã‚¤ãƒ‰ã¯[ã“ã¡ã‚‰](docs/mddocs/Quickstart/llama3_llamacpp_ollama_quickstart.md)ã‚’ã”è¦§ãã ã•ã„ã€‚
- [2024/04] `ipex-llm`ã¯Intel [GPU](python/llm/example/GPU/HuggingFace/LLM/llama3)ãŠã‚ˆã³[CPU](python/llm/example/CPU/HF-Transformers-AutoModels/Model/llama3)ã§**Llama 3**ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚
- [2024/04] `ipex-llm`ã¯C++ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’æä¾›ã—ã¦ãŠã‚Šã€Intel GPUã§[llama.cpp](docs/mddocs/Quickstart/llama_cpp_quickstart.md)ãŠã‚ˆã³[ollama](docs/mddocs/Quickstart/ollama_quickstart.md)ã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã®åŠ é€Ÿãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã¨ã—ã¦ä½¿ç”¨ã§ãã¾ã™ã€‚
- [2024/03] `bigdl-llm`ã¯`ipex-llm`ã«æ”¹åã•ã‚Œã¾ã—ãŸï¼ˆç§»è¡Œã‚¬ã‚¤ãƒ‰ã¯[ã“ã¡ã‚‰](docs/mddocs/Quickstart/bigdl_llm_migration.md)ã‚’ã”è¦§ãã ã•ã„ï¼‰ã€‚å…ƒã®`BigDL`ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯[ã“ã¡ã‚‰](https://github.com/intel-analytics/bigdl-2.x)ã§è¦‹ã¤ã‘ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
- [2024/02] `ipex-llm`ã¯[ModelScope](python/llm/example/GPU/ModelScope-Models)ï¼ˆ[é­”æ­](python/llm/example/CPU/ModelScope-Models)ï¼‰ã‹ã‚‰ç›´æ¥ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‚µãƒãƒ¼ãƒˆã‚’è¿½åŠ ã—ã¾ã—ãŸã€‚
- [2024/02] `ipex-llm`ã¯åˆæœŸã®**INT2**ã‚µãƒãƒ¼ãƒˆã‚’è¿½åŠ ã—ã¾ã—ãŸï¼ˆllama.cpp [IQ2](python/llm/example/GPU/HuggingFace/Advanced-Quantizations/GGUF-IQ2)ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã«åŸºã¥ãï¼‰ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€16GBã®VRAMã‚’æŒã¤Intel GPUã§å¤§è¦æ¨¡ãªLLMï¼ˆä¾‹ï¼šMixtral-8x7Bï¼‰ã‚’å®Ÿè¡Œã™ã‚‹ã“ã¨ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚
- [2024/02] ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯[Text-Generation-WebUI](https://github.com/intel-analytics/text-generation-webui) GUIã‚’é€šã˜ã¦`ipex-llm`ã‚’ä½¿ç”¨ã§ãã¾ã™ã€‚
- [2024/02] `ipex-llm`ã¯*[Self-Speculative Decoding](docs/mddocs/Inference/Self_Speculative_Decoding.md)*ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ãŠã‚Šã€Intel [GPU](python/llm/example/GPU/Speculative-Decoding)ãŠã‚ˆã³[CPU](python/llm/example/CPU/Speculative-Decoding)ã§ã®FP16ãŠã‚ˆã³BF16æ¨è«–ã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚’**ç´„30ï¼…**çŸ­ç¸®ã—ã¾ã™ã€‚
- [2024/02] `ipex-llm`ã¯Intel GPUã§ã®åŒ…æ‹¬çš„ãªLLMå¾®èª¿æ•´ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ï¼ˆ[LoRA](python/llm/example/GPU/LLM-Finetuning/LoRA)ã€[QLoRA](python/llm/example/GPU/LLM-Finetuning/QLoRA)ã€[DPO](python/llm/example/GPU/LLM-Finetuning/DPO)ã€[QA-LoRA](python/llm/example/GPU/LLM-Finetuning/QA-LoRA)ã€[ReLoRA](python/llm/example/GPU/LLM-Finetuning/ReLora)ã‚’å«ã‚€ï¼‰ã€‚
- [2024/01] `ipex-llm` [QLoRA](python/llm/example/GPU/LLM-Finetuning/QLoRA)ã‚’ä½¿ç”¨ã—ã¦ã€8ã¤ã®Intel Max 1550 GPUã§[Standford-Alpaca](python/llm/example/GPU/LLM-Finetuning/QLoRA/alpaca-qlora)ã‚’ä½¿ç”¨ã—ã¦LLaMA2-7Bã‚’**21åˆ†**ã€LLaMA2-70Bã‚’**3.14æ™‚é–“**ã§å¾®èª¿æ•´ã—ã¾ã—ãŸï¼ˆãƒ–ãƒ­ã‚°ã¯[ã“ã¡ã‚‰](https://www.intel.com/content/www/us/en/developer/articles/technical/finetuning-llms-on-intel-gpus-using-bigdl-llm.html)ã‚’ã”è¦§ãã ã•ã„ï¼‰ã€‚
- [2023/12] `ipex-llm`ã¯[ReLoRA](python/llm/example/GPU/LLM-Finetuning/ReLora)ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ï¼ˆè©³ç´°ã¯*["ReLoRA: High-Rank Training Through Low-Rank Updates"](https://arxiv.org/abs/2307.05695)*ã‚’ã”è¦§ãã ã•ã„ï¼‰ã€‚
- [2023/12] `ipex-llm`ã¯Intel [GPU](python/llm/example/GPU/HuggingFace/LLM/mixtral)ãŠã‚ˆã³[CPU](python/llm/example/CPU/HF-Transformers-AutoModels/Model/mixtral)ã§[Mixtral-8x7B](python/llm/example/GPU/HuggingFace/LLM/mixtral)ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚
- [2023/12] `ipex-llm`ã¯[QA-LoRA](python/llm/example/GPU/LLM-Finetuning/QA-LoRA)ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ï¼ˆè©³ç´°ã¯*["QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models"](https://arxiv.org/abs/2309.14717)*ã‚’ã”è¦§ãã ã•ã„ï¼‰ã€‚
- [2023/12] `ipex-llm`ã¯Intel ***GPU***ã§ã®[FP8ãŠã‚ˆã³FP4æ¨è«–](python/llm/example/GPU/HuggingFace/More-Data-Types)ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚
- [2023/11] åˆæœŸã®ã‚µãƒãƒ¼ãƒˆã¨ã—ã¦ã€[GGUF](python/llm/example/GPU/HuggingFace/Advanced-Quantizations/GGUF)ã€[AWQ](python/llm/example/GPU/HuggingFace/Advanced-Quantizations/AWQ)ã€ãŠã‚ˆã³[GPTQ](python/llm/example/GPU/HuggingFace/Advanced-Quantizations/GPTQ)ãƒ¢ãƒ‡ãƒ«ã‚’ç›´æ¥`ipex-llm`ã«ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
- [2023/11] `ipex-llm`ã¯Intel [GPU](python/llm/example/GPU/vLLM-Serving)ãŠã‚ˆã³[CPU](python/llm/example/CPU/vLLM-Serving)ã§ã®[vLLMé€£ç¶šãƒãƒƒãƒå‡¦ç†](python/llm/example/GPU/vLLM-Serving)ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚
- [2023/10] `ipex-llm`ã¯Intel [GPU](python/llm/example/GPU/LLM-Finetuning/QLoRA)ãŠã‚ˆã³[CPU](python/llm/example/CPU/QLoRA-FineTuning)ã§ã®[QLoRAå¾®èª¿æ•´](python/llm/example/GPU/LLM-Finetuning/QLoRA)ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚
- [2023/10] `ipex-llm`ã¯Intel GPUãŠã‚ˆã³CPUã§ã®[FastChatã‚µãƒ¼ãƒ“ã‚¹](python/llm/src/ipex_llm/llm/serving)ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚
- [2023/09] `ipex-llm`ã¯[Intel GPU](python/llm/example/GPU)ï¼ˆiGPUã€Arcã€Flexã€MAXã‚’å«ã‚€ï¼‰ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚
- [2023/09] `ipex-llm` [ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«](https://github.com/intel-analytics/ipex-llm-tutorial)ãŒå…¬é–‹ã•ã‚Œã¾ã—ãŸã€‚
 
</details> 

## `ipex-llm` ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
ä»¥ä¸‹ã«ã€Intel Core UltraãŠã‚ˆã³Intel Arc GPUã§ã®**ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆé€Ÿåº¦**ã‚’ç¤ºã—ã¾ã™[^1]ï¼ˆè©³ç´°ã¯[[2]](https://www.intel.com/content/www/us/en/developer/articles/technical/accelerate-meta-llama3-with-intel-ai-solutions.html)[[3]](https://www.intel.com/content/www/us/en/developer/articles/technical/accelerate-microsoft-phi-3-models-intel-ai-soln.html)[[4]](https://www.intel.com/content/www/us/en/developer/articles/technical/intel-ai-solutions-accelerate-alibaba-qwen2-llms.html)ã‚’ã”è¦§ãã ã•ã„ï¼‰ã€‚

<table width="100%">
  <tr>
    <td>
      <a href="https://llm-assets.readthedocs.io/en/latest/_images/MTL_perf.jpg" target="_blank">
        <img src="https://llm-assets.readthedocs.io/en/latest/_images/MTL_perf.jpg" width=100%; />
      </a>
    </td>
    <td>
      <a href="https://llm-assets.readthedocs.io/en/latest/_images/Arc_perf.jpg" target="_blank">
        <img src="https://llm-assets.readthedocs.io/en/latest/_images/Arc_perf.jpg" width=100%; />
      </a>
    </td>
  </tr>
</table>

`ipex-llm`ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè¡Œã™ã‚‹ã«ã¯ã€[ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¬ã‚¤ãƒ‰](docs/mddocs/Quickstart/benchmark_quickstart.md)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

## `ipex-llm` ãƒ‡ãƒ¢

ä»¥ä¸‹ã¯ã€Intel Iris iGPUã€Intel Core Ultra iGPUã€å˜ä¸€ã‚«ãƒ¼ãƒ‰Arc GPUã€ã¾ãŸã¯è¤‡æ•°ã‚«ãƒ¼ãƒ‰Arc GPUã‚’ä½¿ç”¨ã—ã¦`ipex-llm`ã§ãƒ­ãƒ¼ã‚«ãƒ«LLMã‚’å®Ÿè¡Œã™ã‚‹ãƒ‡ãƒ¢ã§ã™ã€‚

<table width="100%">
  <tr>
    <td align="center" colspan="1"><strong>Intel Iris iGPU</strong></td>
    <td align="center" colspan="1"><strong>Intel Core Ultra iGPU</strong></td>
    <td align="center" colspan="1"><strong>Intel Arc dGPU</strong></td>
    <td align="center" colspan="1"><strong>2-Card Intel Arc dGPUs</strong></td>
  </tr>
  <tr>
    <td>
      <a href="https://llm-assets.readthedocs.io/en/latest/_images/iris_phi3-3.8B_q4_0_llamacpp_long.gif" target="_blank">
        <img src="https://llm-assets.readthedocs.io/en/latest/_images/iris_phi3-3.8B_q4_0_llamacpp_long.gif" width=100%; />
      </a>
    </td>
    <td>
      <a href="https://llm-assets.readthedocs.io/en/latest/_images/mtl_mistral-7B_q4_k_m_ollama.gif" target="_blank">
        <img src="https://llm-assets.readthedocs.io/en/latest/_images/mtl_mistral-7B_q4_k_m_ollama.gif" width=100%; />
      </a>
    </td>
    <td>
      <a href="https://llm-assets.readthedocs.io/en/latest/_images/arc_llama3-8B_fp8_textwebui.gif" target="_blank">
        <img src="https://llm-assets.readthedocs.io/en/latest/_images/arc_llama3-8B_fp8_textwebui.gif" width=100%; />
      </a>
    </td>
    <td>
      <a href="https://llm-assets.readthedocs.io/en/latest/_images/2arc_qwen1.5-32B_fp6_fastchat.gif" target="_blank">
        <img src="https://llm-assets.readthedocs.io/en/latest/_images/2arc_qwen1.5-32B_fp6_fastchat.gif" width=100%; />
      </a>
    </td>
  </tr>
  <tr>
    <td align="center" width="25%">
      <a href="docs/mddocs/Quickstart/llama_cpp_quickstart.md">llama.cpp (Phi-3-mini Q4_0)</a>
    </td>
    <td align="center" width="25%">
      <a href="docs/mddocs/Quickstart/ollama_quickstart.md">Ollama (Mistral-7B Q4_K) </a>
    </td>
    <td align="center" width="25%">
      <a href="docs/mddocs/Quickstart/webui_quickstart.md">TextGeneration-WebUI (Llama3-8B FP8) </a>
    </td>
    <td align="center" width="25%">
      <a href="docs/mddocs/Quickstart/fastchat_quickstart.md">FastChat (QWen1.5-32B FP6)</a>
    </td>  </tr>
</table>

## ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦
ä»¥ä¸‹ã«ã€**Perplexity**ã®çµæœã‚’ç¤ºã—ã¾ã™ï¼ˆWikitextãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã—ã¦[ã“ã¡ã‚‰](https://github.com/intel-analytics/ipex-llm/tree/main/python/llm/dev/benchmark/perplexity)ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½¿ç”¨ã—ã¦ãƒ†ã‚¹ãƒˆã—ã¾ã—ãŸï¼‰ã€‚
|Perplexity                 |sym_int4	|q4_k	  |fp6	  |fp8_e5m2 |fp8_e4m3 |fp16   |
|---------------------------|---------|-------|-------|---------|---------|-------|
|Llama-2-7B-chat-hf	        |6.364 	  |6.218 	|6.092 	|6.180 	  |6.098    |6.096  | 
|Mistral-7B-Instruct-v0.2	  |5.365 	  |5.320 	|5.270 	|5.273 	  |5.246	   |5.244  |
|Baichuan2-7B-chat	         |6.734    |6.727	 |6.527	 |6.539	   |6.488	   |6.508  |
|Qwen1.5-7B-chat	           |8.865 	  |8.816 	|8.557 	|8.846 	  |8.530    |8.607  | 
|Llama-3.1-8B-Instruct	     |6.705	   |6.566	 |6.338	 |6.383	   |6.325	   |6.267  |
|gemma-2-9b-it	             |7.541	   |7.412	 |7.269	 |7.380	   |7.268	   |7.270  |
|Baichuan2-13B-Chat	        |6.313	   |6.160	 |6.070	 |6.145	   |6.086	   |6.031  |
|Llama-2-13b-chat-hf	       |5.449	   |5.422	 |5.341	 |5.384	   |5.332	   |5.329  |
|Qwen1.5-14B-Chat	          |7.529	   |7.520	 |7.367	 |7.504	   |7.297	   |7.334  |

[^1]: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¯ä½¿ç”¨æ–¹æ³•ã€æ§‹æˆã€ãŠã‚ˆã³ãã®ä»–ã®è¦å› ã«ã‚ˆã£ã¦ç•°ãªã‚Šã¾ã™ã€‚`ipex-llm`ã¯ã€éIntelè£½å“ã«å¯¾ã—ã¦åŒã˜ç¨‹åº¦ã®æœ€é©åŒ–ã‚’è¡Œã‚ãªã„å ´åˆãŒã‚ã‚Šã¾ã™ã€‚è©³ç´°ã¯www.Intel.com/PerformanceIndexã‚’ã”è¦§ãã ã•ã„ã€‚

## `ipex-llm` ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

### Docker
- [GPU Inference in C++](docs/mddocs/DockerGuides/docker_cpp_xpu_quickstart.md): Intel GPUã§`ipex-llm`ã‚’ä½¿ç”¨ã—ã¦`llama.cpp`ã€`ollama`ãªã©ã‚’å®Ÿè¡Œã™ã‚‹
- [GPU Inference in Python](docs/mddocs/DockerGuides/docker_pytorch_inference_gpu.md): Intel GPUã§`ipex-llm`ã‚’ä½¿ç”¨ã—ã¦HuggingFace `transformers`ã€`LangChain`ã€`LlamaIndex`ã€`ModelScope`ãªã©ã‚’å®Ÿè¡Œã™ã‚‹
- [vLLM on GPU](docs/mddocs/DockerGuides/vllm_docker_quickstart.md): Intel GPUã§`ipex-llm`ã‚’ä½¿ç”¨ã—ã¦`vLLM`æ¨è«–ã‚µãƒ¼ãƒ“ã‚¹ã‚’å®Ÿè¡Œã™ã‚‹
- [vLLM on CPU](docs/mddocs/DockerGuides/vllm_cpu_docker_quickstart.md): Intel CPUã§`ipex-llm`ã‚’ä½¿ç”¨ã—ã¦`vLLM`æ¨è«–ã‚µãƒ¼ãƒ“ã‚¹ã‚’å®Ÿè¡Œã™ã‚‹
- [FastChat on GPU](docs/mddocs/DockerGuides/fastchat_docker_quickstart.md): Intel GPUã§`ipex-llm`ã‚’ä½¿ç”¨ã—ã¦`FastChat`æ¨è«–ã‚µãƒ¼ãƒ“ã‚¹ã‚’å®Ÿè¡Œã™ã‚‹
- [VSCode on GPU](docs/mddocs/DockerGuides/docker_run_pytorch_inference_in_vscode.md): Intel GPUã§VSCodeã‚’ä½¿ç”¨ã—ã¦Pythonãƒ™ãƒ¼ã‚¹ã®`ipex-llm`ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é–‹ç™ºãŠã‚ˆã³å®Ÿè¡Œã™ã‚‹

### ä½¿ç”¨
- [llama.cpp](docs/mddocs/Quickstart/llama_cpp_quickstart.md): Intel GPUã§**llama.cpp**ã‚’å®Ÿè¡Œã™ã‚‹ï¼ˆ`ipex-llm`ã®C++ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’ä½¿ç”¨ï¼‰
- [Ollama](docs/mddocs/Quickstart/ollama_quickstart.md): Intel GPUã§**ollama**ã‚’å®Ÿè¡Œã™ã‚‹ï¼ˆ`ipex-llm`ã®C++ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’ä½¿ç”¨ï¼‰
- [PyTorch/HuggingFace](docs/mddocs/Quickstart/install_windows_gpu.md): Intel GPUã§**PyTorch**ã€**HuggingFace**ã€**LangChain**ã€**LlamaIndex**ãªã©ã‚’å®Ÿè¡Œã™ã‚‹ï¼ˆ`ipex-llm`ã®Pythonã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’ä½¿ç”¨ï¼‰[Windows](docs/mddocs/Quickstart/install_windows_gpu.md)ãŠã‚ˆã³[Linux](docs/mddocs/Quickstart/install_linux_gpu.md)
- [vLLM](docs/mddocs/Quickstart/vLLM_quickstart.md): Intel [GPU](docs/mddocs/DockerGuides/vllm_docker_quickstart.md)ãŠã‚ˆã³[CPU](docs/mddocs/DockerGuides/vllm_cpu_docker_quickstart.md)ã§`ipex-llm`ã‚’ä½¿ç”¨ã—ã¦**vLLM**ã‚’å®Ÿè¡Œã™ã‚‹
- [FastChat](docs/mddocs/Quickstart/fastchat_quickstart.md): Intel GPUãŠã‚ˆã³CPUã§`ipex-llm`ã‚’ä½¿ç”¨ã—ã¦**FastChat**ã‚µãƒ¼ãƒ“ã‚¹ã‚’å®Ÿè¡Œã™ã‚‹
- [Serving on multiple Intel GPUs](docs/mddocs/Quickstart/deepspeed_autotp_fastapi_quickstart.md): DeepSpeed AutoTPãŠã‚ˆã³FastAPIã‚’æ´»ç”¨ã—ã¦è¤‡æ•°ã®Intel GPUã§`ipex-llm`**æ¨è«–ã‚µãƒ¼ãƒ“ã‚¹**ã‚’å®Ÿè¡Œã™ã‚‹
- [Text-Generation-WebUI](docs/mddocs/Quickstart/webui_quickstart.md): `ipex-llm`ã‚’ä½¿ç”¨ã—ã¦`oobabooga`**WebUI**ã‚’å®Ÿè¡Œã™ã‚‹
- [Axolotl](docs/mddocs/Quickstart/axolotl_quickstart.md): **Axolotl**ã§`ipex-llm`ã‚’ä½¿ç”¨ã—ã¦LLMã‚’å¾®èª¿æ•´ã™ã‚‹
- [Benchmarking](docs/mddocs/Quickstart/benchmark_quickstart.md): Intel GPUãŠã‚ˆã³CPUã§`ipex-llm`ã®**ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯**ï¼ˆãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŠã‚ˆã³ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆï¼‰ã‚’å®Ÿè¡Œã™ã‚‹

### ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
- [GraphRAG](docs/mddocs/Quickstart/graphrag_quickstart.md): `ipex-llm`ã‚’ä½¿ç”¨ã—ã¦Microsoftã®`GraphRAG`ã‚’ãƒ­ãƒ¼ã‚«ãƒ«LLMã§å®Ÿè¡Œã™ã‚‹
- [RAGFlow](docs/mddocs/Quickstart/ragflow_quickstart.md): `ipex-llm`ã‚’ä½¿ç”¨ã—ã¦`RAGFlow`ï¼ˆ*ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã®RAGã‚¨ãƒ³ã‚¸ãƒ³*ï¼‰ã‚’å®Ÿè¡Œã™ã‚‹
- [LangChain-Chatchat](docs/mddocs/Quickstart/chatchat_quickstart.md): `ipex-llm`ã‚’ä½¿ç”¨ã—ã¦`LangChain-Chatchat`ï¼ˆ*RAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½¿ç”¨ã—ãŸãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹QA*ï¼‰ã‚’å®Ÿè¡Œã™ã‚‹
- [Coding copilot](docs/mddocs/Quickstart/continue_quickstart.md): `ipex-llm`ã‚’ä½¿ç”¨ã—ã¦`Continue`ï¼ˆVSCodeã®ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚³ãƒ‘ã‚¤ãƒ­ãƒƒãƒˆï¼‰ã‚’å®Ÿè¡Œã™ã‚‹
- [Open WebUI](docs/mddocs/Quickstart/open_webui_with_ollama_quickstart.md): `ipex-llm`ã‚’ä½¿ç”¨ã—ã¦`Open WebUI`ã‚’å®Ÿè¡Œã™ã‚‹
- [PrivateGPT](docs/mddocs/Quickstart/privateGPT_quickstart.md): `ipex-llm`ã‚’ä½¿ç”¨ã—ã¦`PrivateGPT`ã‚’å®Ÿè¡Œã—ã€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨å¯¾è©±ã™ã‚‹
- [Dify platform](docs/mddocs/Quickstart/dify_quickstart.md): `Dify`ï¼ˆ*ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³å¯¾å¿œã®LLMã‚¢ãƒ—ãƒªé–‹ç™ºãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ *ï¼‰ã§`ipex-llm`ã‚’ä½¿ç”¨ã™ã‚‹

### ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
- [Windows GPU](docs/mddocs/Quickstart/install_windows_gpu.md): Intel GPUã‚’æ­è¼‰ã—ãŸWindowsã§`ipex-llm`ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹
- [Linux GPU](docs/mddocs/Quickstart/install_linux_gpu.md): Intel GPUã‚’æ­è¼‰ã—ãŸLinuxã§`ipex-llm`ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹
- *è©³ç´°ã«ã¤ã„ã¦ã¯ã€[å®Œå…¨ãªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚¬ã‚¤ãƒ‰](docs/mddocs/Overview/install.md)ã‚’å‚ç…§ã—ã¦ãã ã•ã„*

### ã‚³ãƒ¼ãƒ‰ä¾‹
- #### ä½ãƒ“ãƒƒãƒˆæ¨è«–
  - [INT4æ¨è«–](python/llm/example/GPU/HuggingFace/LLM): Intel [GPU](python/llm/example/GPU/HuggingFace/LLM)ãŠã‚ˆã³[CPU](python/llm/example/CPU/HF-Transformers-AutoModels/Model)ã§**INT4** LLMæ¨è«–ã‚’å®Ÿè¡Œã™ã‚‹
  - [FP8/FP6/FP4æ¨è«–](python/llm/example/GPU/HuggingFace/More-Data-Types): Intel [GPU](python/llm/example/GPU/HuggingFace/More-Data-Types)ã§**FP8**ã€**FP6**ã€**FP4** LLMæ¨è«–ã‚’å®Ÿè¡Œã™ã‚‹
  - [INT8æ¨è«–](python/llm/example/GPU/HuggingFace/More-Data-Types): Intel [GPU](python/llm/example/GPU/HuggingFace/More-Data-Types)ãŠã‚ˆã³[CPU](python/llm/example/CPU/HF-Transformers-AutoModels/More-Data-Types)ã§**INT8** LLMæ¨è«–ã‚’å®Ÿè¡Œã™ã‚‹
  - [INT2æ¨è«–](python/llm/example/GPU/HuggingFace/Advanced-Quantizations/GGUF-IQ2): Intel [GPU](python/llm/example/GPU/HuggingFace/Advanced-Quantizations/GGUF-IQ2)ã§**INT2** LLMæ¨è«–ã‚’å®Ÿè¡Œã™ã‚‹ï¼ˆllama.cpp IQ2ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã«åŸºã¥ãï¼‰
- #### FP16/BF16æ¨è«–
  - Intel [GPU](python/llm/example/GPU/Speculative-Decoding)ã§**FP16** LLMæ¨è«–ã‚’å®Ÿè¡Œã™ã‚‹ï¼ˆ[self-speculative decoding](docs/mddocs/Inference/Self_Speculative_Decoding.md)æœ€é©åŒ–ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆï¼‰
  - Intel [CPU](python/llm/example/CPU/Speculative-Decoding)ã§**BF16** LLMæ¨è«–ã‚’å®Ÿè¡Œã™ã‚‹ï¼ˆ[self-speculative decoding](docs/mddocs/Inference/Self_Speculative_Decoding.md)æœ€é©åŒ–ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆï¼‰
- #### åˆ†æ•£æ¨è«–
  - Intel [GPU](python/llm/example/GPU/Pipeline-Parallel-Inference)ã§**ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä¸¦åˆ—**æ¨è«–ã‚’å®Ÿè¡Œã™ã‚‹
  - Intel [GPU](python/llm/example/GPU/Deepspeed-AutoTP)ã§**DeepSpeed AutoTP**æ¨è«–ã‚’å®Ÿè¡Œã™ã‚‹
- #### ä¿å­˜ã¨èª­ã¿è¾¼ã¿
  - [ä½ãƒ“ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«](python/llm/example/CPU/HF-Transformers-AutoModels/Save-Load): `ipex-llm`ä½ãƒ“ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ï¼ˆINT4/FP4/FP6/INT8/FP8/FP16/etc.ï¼‰ã‚’ä¿å­˜ãŠã‚ˆã³èª­ã¿è¾¼ã‚€
  - [GGUF](python/llm/example/GPU/HuggingFace/Advanced-Quantizations/GGUF): GGUFãƒ¢ãƒ‡ãƒ«ã‚’ç›´æ¥`ipex-llm`ã«èª­ã¿è¾¼ã‚€
  - [AWQ](python/llm/example/GPU/HuggingFace/Advanced-Quantizations/AWQ): AWQãƒ¢ãƒ‡ãƒ«ã‚’ç›´æ¥`ipex-llm`ã«èª­ã¿è¾¼ã‚€
  - [GPTQ](python/llm/example/GPU/HuggingFace/Advanced-Quantizations/GPTQ): GPTQãƒ¢ãƒ‡ãƒ«ã‚’ç›´æ¥`ipex-llm`ã«èª­ã¿è¾¼ã‚€
- #### å¾®èª¿æ•´
  - Intel [GPU](python/llm/example/GPU/LLM-Finetuning)ã§LLMã‚’å¾®èª¿æ•´ã™ã‚‹ï¼ˆ[LoRA](python/llm/example/GPU/LLM-Finetuning/LoRA)ã€[QLoRA](python/llm/example/GPU/LLM-Finetuning/QLoRA)ã€[DPO](python/llm/example/GPU/LLM-Finetuning/DPO)ã€[QA-LoRA](python/llm/example/GPU/LLM-Finetuning/QA-LoRA)ã€[ReLoRA](python/llm/example/GPU/LLM-Finetuning/ReLora)ã‚’å«ã‚€ï¼‰
  - Intel [CPU](python/llm/example/CPU/QLoRA-FineTuning)ã§QLoRAã‚’å¾®èª¿æ•´ã™ã‚‹
- #### ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¨ã®çµ±åˆ
  - [HuggingFace transformers](python/llm/example/GPU/HuggingFace)
  - [Standard PyTorch model](python/llm/example/GPU/PyTorch-Models)
  - [LangChain](python/llm/example/GPU/LangChain)
  - [LlamaIndex](python/llm/example/GPU/LlamaIndex)
  - [DeepSpeed-AutoTP](python/llm/example/GPU/Deepspeed-AutoTP)
  - [Axolotl](docs/mddocs/Quickstart/axolotl_quickstart.md)
  - [HuggingFace PEFT](python/llm/example/GPU/LLM-Finetuning/HF-PEFT)
  - [HuggingFace TRL](python/llm/example/GPU/LLM-Finetuning/DPO)
  - [AutoGen](python/llm/example/CPU/Applications/autogen)
  - [ModeScope](python/llm/example/GPU/ModelScope-Models)
- [ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«](https://github.com/intel-analytics/ipex-llm-tutorial)

## APIãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
- [HuggingFace Transformersã‚¹ã‚¿ã‚¤ãƒ«ã®APIï¼ˆAutoã‚¯ãƒ©ã‚¹ï¼‰](docs/mddocs/PythonAPI/transformers.md)
- [ä»»æ„ã®PyTorchãƒ¢ãƒ‡ãƒ«ç”¨ã®API](https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/PythonAPI/optimize.md)

## FAQ
- [FAQã¨ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°](docs/mddocs/Overview/FAQ/faq.md)

## æ¤œè¨¼æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
50ä»¥ä¸Šã®ãƒ¢ãƒ‡ãƒ«ãŒ`ipex-llm`ã§æœ€é©åŒ–/æ¤œè¨¼ã•ã‚Œã¦ãŠã‚Šã€*LLaMA/LLaMA2ã€Mistralã€Mixtralã€Gemmaã€LLaVAã€Whisperã€ChatGLM2/ChatGLM3ã€Baichuan/Baichuan2ã€Qwen/Qwen-1.5ã€InternLM*ãªã©ãŒå«ã¾ã‚Œã¾ã™ã€‚è©³ç´°ã¯ä»¥ä¸‹ã®ãƒªã‚¹ãƒˆã‚’ã”è¦§ãã ã•ã„ã€‚
  
| ãƒ¢ãƒ‡ãƒ«      | CPUä¾‹                                                    | GPUä¾‹                                                     |
|------------|----------------------------------------------------------------|-----------------------------------------------------------------|
| LLaMA *(such as Vicuna, Guanaco, Koala, Baize, WizardLM, etc.)* | [link1](python/llm/example/CPU/Native-Models), [link2](python/llm/example/CPU/HF-Transformers-AutoModels/Model/vicuna) |[link](python/llm/example/GPU/HuggingFace/LLM/vicuna)|
| LLaMA 2    | [link1](python/llm/example/CPU/Native-Models), [link2](python/llm/example/CPU/HF-Transformers-AutoModels/Model/llama2) | [link](python/llm/example/GPU/HuggingFace/LLM/llama2)  |
| LLaMA 3    | [link](python/llm/example/CPU/HF-Transformers-AutoModels/Model/llama3) | [link](python/llm/example/GPU/HuggingFace/LLM/llama3)  |
| LLaMA 3.1    | [link](python/llm/example/CPU/HF-Transformers-AutoModels/Model/llama3.1) | [link](python/llm/example/GPU/HuggingFace/LLM/llama3.1)  |
| LLaMA 3.2    |  | [link](python/llm/example/GPU/HuggingFace/LLM/llama3.2)  |
| LLaMA 3.2-Vision    |  | [link](python/llm/example/GPU/PyTorch-Models/Model/llama3.2-vision/)  |
| ChatGLM    | [link](python/llm/example/CPU/HF-Transformers-AutoModels/Model/chatglm)   |    | 
| ChatGLM2   | [link](python/llm/example/CPU/HF-Transformers-AutoModels/Model/chatglm2)  | [link](python/llm/example/GPU/HuggingFace/LLM/chatglm2)   |
| ChatGLM3   | [link](python/llm/example/CPU/HF-Transformers-AutoModels/Model/chatglm3)  | [link](python/llm/example/GPU/HuggingFace/LLM/chatglm3)   |
| GLM-4      | [link](python/llm/example/CPU/HF-Transformers-AutoModels/Model/glm4)      | [link](python/llm/example/GPU/HuggingFace/LLM/glm4)       |
| GLM-4V     | [link](python/llm/example/CPU/HF-Transformers-AutoModels/Model/glm-4v)    | [link](python/llm/example/GPU/HuggingFace/Multimodal/glm-4v)     |
| Mistral    | [link](python/llm/example/CPU/HF-Transformers-AutoModels/Model/mistral)   | [link](python/llm/example/GPU/HuggingFace/LLM/mistral)    |
| Mixtral    | [link](python/llm/example/CPU/HF-Transformers-AutoModels/Model/mixtral)   | [link](python/llm/example/GPU/HuggingFace/LLM/mixtral)    |
| Falcon     | [link](python/llm/example/CPU/HF-Transformers-AutoModels/Model/falcon)    | [link](python/llm/example/GPU/HuggingFace/LLM/falcon)     |
| MPT        | [link](python/llm/example/CPU/HF-Transformers-AutoModels/Model/mpt)       | [link](python/llm/example/GPU/HuggingFace/LLM/mpt)        |
| Dolly-v1   | [link](python/llm/example/CPU/HF-Transformers-AutoModels/Model/dolly_v1)  | [link](python/llm/example/GPU/HuggingFace/LLM/dolly-v1)   | 
| Dolly-v2   | [link](python/llm/example/CPU/HF-Transformers-AutoModels/Model/dolly_v2)  | [link](python/llm/example/GPU/HuggingFace/LLM/dolly-v2)   | 
| Replit Code| [link](python/llm/example/CPU/HF-Transformers-AutoModels/Model/replit)    | [link](python/llm/example/GPU/HuggingFace/LLM/replit)     |
| RedPajama  | [link1](python/llm/example/CPU/Native-Models), [link2](python/llm/example/CPU/HF-Transformers-AutoModels/Model/redpajama) |    | 
| Phoenix    | [link1](python/llm/example/CPU/Native-Models), [link2](python/llm/example/CPU/HF-Transformers-AutoModels/Model/phoenix)   |    | 
| StarCoder  | [link1](python/llm/example/CPU/Native-Models), [link2](python/llm/example/CPU/HF-Transformers-AutoModels/Model/starcoder) | [link](python/llm/example/GPU/HuggingFace/LLM/starcoder) | 
| Baichuan   | [link](python/llm/example/CPU/HF-Transformers-AutoModels/Model/baichuan)  | [link](python/llm/example/GPU/HuggingFace/LLM/baichuan)   |
| Baichuan2  | [link](python/llm/example/CPU/HF-Transformers-AutoModels/Model/baichuan2) | [link](python/llm/example/GPU/HuggingFace/LLM/baichuan2)  |
| InternLM   | [link](python/llm/example/CPU/HF-Transformers-AutoModels/Model/internlm)  | [link](python/llm/example/GPU/HuggingFace/LLM/internlm)   |
| InternVL2   |   | [link](python/llm/example/GPU/HuggingFace/Multimodal/internvl2)   |
| Qwen       | [link](python/llm/example/CPU/HF-Transformers-AutoModels/Model/qwen)      | [link](python/llm/example/GPU/HuggingFace/LLM/qwen)       |
| Qwen1.5 | [link](python/llm/example/CPU/HF-Transformers-AutoModels/Model/qwen1.5) | [link](python/llm/example/GPU/HuggingFace/LLM/qwen1.5) |
| Qwen2 | [link](python/llm/example/CPU/HF-Transformers-AutoModels/Model/qwen2) | [link](python/llm/example/GPU/HuggingFace/LLM/qwen2) |
| Qwen2.5 |  | [link](python/llm/example/GPU/HuggingFace/LLM/qwen2.5) |
| Qwen-VL    | [link](python/llm/example
