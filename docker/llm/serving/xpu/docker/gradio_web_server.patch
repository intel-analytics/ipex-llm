--- gradio_web_server.py
+++ gradio_web_server_new.py
@@ -362,9 +362,9 @@ def handle_latency_metrics(first_token_time, next_token_time):
     first_token_latency = "None"
     next_token_latency = "None"
     if first_token_time is not None:
-        first_token_latency = str(first_token_time * 1000) + " ms"
+        first_token_latency = f"{first_token_time * 1000 :.2f} ms"
     if next_token_time.size > 0:
-        next_token_latency = str(np.mean(next_token_time) * 1000) + " ms"
+        next_token_latency = f"{np.mean(next_token_time) * 1000 :.2f} ms"
     return first_token_latency, next_token_latency
 
 
@@ -725,26 +725,6 @@ def build_single_model_ui(models, add_promotion_links=False):
         regenerate_btn = gr.Button(value="üîÑ  Regenerate", interactive=False)
         clear_btn = gr.Button(value="üóëÔ∏è  Clear history", interactive=False)
 
-    with gr.Row():
-        with gr.Column():
-            gr.Markdown("### Performance Metrics")
-            prompt_token = gr.Textbox(
-                label="Prompt token length:",
-                interactive=False,
-            )
-            next_token = gr.Textbox(
-                label="Generated token length:",
-                interactive=False,
-            )
-            first_token_latency = gr.Textbox(
-                interactive=False,
-                label="First token Latency:",
-            )
-            next_token_latency = gr.Textbox(
-                interactive=False,
-                label="Next token Latency:",
-            )
-
     with gr.Accordion("Parameters", open=False) as parameter_row:
         temperature = gr.Slider(
             minimum=0.0,
@@ -771,6 +751,22 @@ def build_single_model_ui(models, add_promotion_links=False):
             label="Max output tokens",
         )
 
+    with gr.Row():
+        with gr.Column():
+            gr.Markdown("### Performance Metrics")
+            prompt_token = gr.Label(
+                label="Prompt token length:",
+            )
+            next_token = gr.Label(
+                label="Generated token length:",
+            )
+            first_token_latency = gr.Label(
+                label="First token Latency:",
+            )
+            next_token_latency = gr.Label(
+                label="Next token Latency:",
+            )
+
     if add_promotion_links:
         gr.Markdown(acknowledgment_md, elem_id="ack_markdown")
 
     with gr.Blocks(
-        title="Chat with Open Large Language Models",
+        title="ChatBot based Xeon-W & Arc GPUs",
         theme=gr.themes.Default(),
         css=block_css,
     ) as demo:
     with gr.Blocks(
-        title="Chat with Open Large Language Models",
+        title="ChatBot based Xeon-W & Arc GPUs",
         theme=gr.themes.Default(),
         css=block_css,
     ) as demo:
     with gr.Blocks(
-        title="Chat with Open Large Language Models",
+        title="ChatBot based Xeon-W & Arc GPUs",
         theme=gr.themes.Default(),
         css=block_css,
     ) as demo:
     with gr.Blocks(
-        title="Chat with Open Large Language Models",
+        title="ChatBot based Xeon-W & Arc GPUs",
         theme=gr.themes.Default(),
         css=block_css,
     ) as demo:
     with gr.Blocks(
-        title="Chat with Open Large Language Models",
+        title="ChatBot based Xeon-W & Arc GPUs",
         theme=gr.themes.Default(),
         css=block_css,
     ) as demo:
     with gr.Blocks(
-        title="Chat with Open Large Language Models",
+        title="ChatBot based Xeon-W & Arc GPUs",
         theme=gr.themes.Default(),
         css=block_css,
     ) as demo:
