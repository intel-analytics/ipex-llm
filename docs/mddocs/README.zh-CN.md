#  ğŸ’« IntelÂ® LLM Library for PyTorch* 
<p>
  < <a href='./README.md'>English</a> | <b>ä¸­æ–‡ ></b> 
</p>

## æœ€æ–°æ›´æ–° ğŸ”¥ 
- [2025/02] æ–°å¢ [Ollama Portable Zip](https://github.com/intel/ipex-llm/releases/tag/v2.2.0-nightly) åœ¨ Intel GPU ä¸Šç›´æ¥**å…å®‰è£…è¿è¡Œ Ollama** (åŒ…æ‹¬ [Windows](Quickstart/ollama_portable_zip_quickstart.zh-CN.md#windowsç”¨æˆ·æŒ‡å—) å’Œ [Linux](Quickstart/ollama_portable_zip_quickstart.zh-CN.md#linuxç”¨æˆ·æŒ‡å—))ã€‚
- [2025/02] æ–°å¢åœ¨ Intel Arc GPUs ä¸Šè¿è¡Œ [vLLM 0.6.6](DockerGuides/vllm_docker_quickstart.md) çš„æ”¯æŒã€‚
- [2025/01] æ–°å¢åœ¨ Intel Arc [B580](Quickstart/bmg_quickstart.md) GPU ä¸Šè¿è¡Œ `ipex-llm` çš„æŒ‡å—ã€‚
- [2025/01] æ–°å¢åœ¨ Intel GPU ä¸Šè¿è¡Œ [Ollama 0.5.4](Quickstart/ollama_quickstart.zh-CN.md) çš„æ”¯æŒã€‚
- [2024/12] å¢åŠ äº†å¯¹ Intel Core Ultra [NPU](Quickstart/npu_quickstart.md)ï¼ˆåŒ…æ‹¬ 100Hï¼Œ200V å’Œ 200K ç³»åˆ—ï¼‰çš„ **Python** å’Œ **C++** æ”¯æŒã€‚

<details><summary>æ›´å¤šæ›´æ–°</summary>
<br/>

- [2024/11] æ–°å¢åœ¨ Intel Arc GPUs ä¸Šè¿è¡Œ [vLLM 0.6.2](DockerGuides/vllm_docker_quickstart.md) çš„æ”¯æŒã€‚
- [2024/07] æ–°å¢ Microsoft **GraphRAG** çš„æ”¯æŒï¼ˆä½¿ç”¨è¿è¡Œåœ¨æœ¬åœ° Intel GPU ä¸Šçš„ LLMï¼‰ï¼Œè¯¦æƒ…å‚è€ƒ[å¿«é€Ÿå…¥é—¨æŒ‡å—](Quickstart/graphrag_quickstart.md)ã€‚
- [2024/07] å…¨é¢å¢å¼ºäº†å¯¹å¤šæ¨¡æ€å¤§æ¨¡å‹çš„æ”¯æŒï¼ŒåŒ…æ‹¬ [StableDiffusion](../../python/llm/example/GPU/HuggingFace/Multimodal/StableDiffusion), [Phi-3-Vision](../../python/llm/example/GPU/HuggingFace/Multimodal/phi-3-vision), [Qwen-VL](../../python/llm/example/GPU/HuggingFace/Multimodal/qwen-vl)ï¼Œæ›´å¤šè¯¦æƒ…è¯·ç‚¹å‡»[è¿™é‡Œ](../../python/llm/example/GPU/HuggingFace/Multimodal)ã€‚
- [2024/07] æ–°å¢ Intel GPU ä¸Š **FP6** çš„æ”¯æŒï¼Œè¯¦æƒ…å‚è€ƒ[æ›´å¤šæ•°æ®ç±»å‹æ ·ä¾‹](../../python/llm/example/GPU/HuggingFace/More-Data-Types)ã€‚ 
- [2024/06] æ–°å¢å¯¹ Intel Core Ultra å¤„ç†å™¨ä¸­ **NPU** çš„å®éªŒæ€§æ”¯æŒï¼Œè¯¦æƒ…å‚è€ƒ[ç›¸å…³ç¤ºä¾‹](../../python/llm/example/NPU/HF-Transformers-AutoModels)ã€‚ 
- [2024/06] å¢åŠ äº†å¯¹[æµæ°´çº¿å¹¶è¡Œæ¨ç†](../../python/llm/example/GPU/Pipeline-Parallel-Inference)çš„å…¨é¢æ”¯æŒï¼Œä½¿å¾—ç”¨ä¸¤å—æˆ–æ›´å¤š Intel GPUï¼ˆå¦‚ Arcï¼‰ä¸Šè¿è¡Œ LLM å˜å¾—æ›´å®¹æ˜“ã€‚
- [2024/06] æ–°å¢åœ¨ Intel GPU ä¸Šè¿è¡Œ **RAGFlow** çš„æ”¯æŒï¼Œè¯¦æƒ…å‚è€ƒ[å¿«é€Ÿå…¥é—¨æŒ‡å—](Quickstart/ragflow_quickstart.md)ã€‚
- [2024/05] æ–°å¢ **Axolotl** çš„æ”¯æŒï¼Œå¯ä»¥åœ¨ Intel GPU ä¸Šè¿›è¡ŒLLMå¾®è°ƒï¼Œè¯¦æƒ…å‚è€ƒ[å¿«é€Ÿå…¥é—¨æŒ‡å—](Quickstart/axolotl_quickstart.md)ã€‚
- [2024/05] ä½ å¯ä»¥ä½¿ç”¨ **Docker** [images](#docker) å¾ˆå®¹æ˜“åœ°è¿è¡Œ `ipex-llm` æ¨ç†ã€æœåŠ¡å’Œå¾®è°ƒã€‚
- [2024/05] ä½ èƒ½å¤Ÿåœ¨ Windows ä¸Šä»…ä½¿ç”¨ "*[one command](Quickstart/install_windows_gpu.zh-CN.md#å®‰è£…-ipex-llm)*" æ¥å®‰è£… `ipex-llm`ã€‚
- [2024/04] ä½ ç°åœ¨å¯ä»¥åœ¨ Intel GPU ä¸Šä½¿ç”¨ `ipex-llm` è¿è¡Œ **Open WebUI** ï¼Œè¯¦æƒ…å‚è€ƒ[å¿«é€Ÿå…¥é—¨æŒ‡å—](Quickstart/open_webui_with_ollama_quickstart.md)ã€‚
- [2024/04] ä½ ç°åœ¨å¯ä»¥åœ¨ Intel GPU ä¸Šä½¿ç”¨ `ipex-llm` ä»¥åŠ `llama.cpp` å’Œ `ollama` è¿è¡Œ **Llama 3** ï¼Œè¯¦æƒ…å‚è€ƒ[å¿«é€Ÿå…¥é—¨æŒ‡å—](Quickstart/llama3_llamacpp_ollama_quickstart.md)ã€‚
- [2024/04] `ipex-llm` ç°åœ¨åœ¨Intel [GPU](../../python/llm/example/GPU/HuggingFace/LLM/llama3) å’Œ [CPU](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/llama3) ä¸Šéƒ½æ”¯æŒ **Llama 3** äº†ã€‚
- [2024/04] `ipex-llm` ç°åœ¨æä¾› C++ æ¨ç†, åœ¨ Intel GPU ä¸Šå®ƒå¯ä»¥ç”¨ä½œè¿è¡Œ [llama.cpp](Quickstart/llama_cpp_quickstart.zh-CN.md) å’Œ [ollama](Quickstart/ollama_quickstart.zh-CN.md) çš„åŠ é€Ÿåç«¯ã€‚
- [2024/03] `bigdl-llm` ç°å·²æ›´åä¸º `ipex-llm` (è¯·å‚é˜…[æ­¤å¤„](Quickstart/bigdl_llm_migration.md)çš„è¿ç§»æŒ‡å—)ï¼Œä½ å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/intel-analytics/bigdl-2.x)æ‰¾åˆ°åŸå§‹BigDLé¡¹ç›®ã€‚
- [2024/02] `ipex-llm` ç°åœ¨æ”¯æŒç›´æ¥ä» [ModelScope](../../python/llm/example/GPU/ModelScope-Models) ([é­”æ­](../../python/llm/example/CPU/ModelScope-Models)) loading æ¨¡å‹ã€‚
- [2024/02] `ipex-llm` å¢åŠ  **INT2** çš„æ”¯æŒ (åŸºäº llama.cpp [IQ2](../../python/llm/example/GPU/HuggingFace/Advanced-Quantizations/GGUF-IQ2) æœºåˆ¶), è¿™ä½¿å¾—åœ¨å…·æœ‰ 16GB VRAM çš„ Intel GPU ä¸Šè¿è¡Œå¤§å‹ LLMï¼ˆä¾‹å¦‚ Mixtral-8x7Bï¼‰æˆä¸ºå¯èƒ½ã€‚
- [2024/02] ç”¨æˆ·ç°åœ¨å¯ä»¥é€šè¿‡ [Text-Generation-WebUI](https://github.com/intel-analytics/text-generation-webui) GUI ä½¿ç”¨ `ipex-llm`ã€‚
- [2024/02] `ipex-llm` ç°åœ¨æ”¯æŒ *[Self-Speculative Decoding](Inference/Self_Speculative_Decoding.md)*ï¼Œè¿™ä½¿å¾—åœ¨ Intel [GPU](../../python/llm/example/GPU/Speculative-Decoding) å’Œ [CPU](../../python/llm/example/CPU/Speculative-Decoding) ä¸Šä¸º FP16 å’Œ BF16 æ¨ç†å¸¦æ¥ **~30% åŠ é€Ÿ** ã€‚
- [2024/02] `ipex-llm` ç°åœ¨æ”¯æŒåœ¨ Intel GPU ä¸Šè¿›è¡Œå„ç§ LLM å¾®è°ƒ(åŒ…æ‹¬ [LoRA](../../python/llm/example/GPU/LLM-Finetuning/LoRA), [QLoRA](../../python/llm/example/GPU/LLM-Finetuning/QLoRA), [DPO](../../python/llm/example/GPU/LLM-Finetuning/DPO), [QA-LoRA](../../python/llm/example/GPU/LLM-Finetuning/QA-LoRA) å’Œ [ReLoRA](../../python/llm/example/GPU/LLM-Finetuning/ReLora))ã€‚
- [2024/01] ä½¿ç”¨ `ipex-llm` [QLoRA](../../python/llm/example/GPU/LLM-Finetuning/QLoRA)ï¼Œæˆ‘ä»¬æˆåŠŸåœ°åœ¨ 8 ä¸ª Intel Max 1550 GPU ä¸Šä½¿ç”¨ [Standford-Alpaca](../../python/llm/example/GPU/LLM-Finetuning/QLoRA/alpaca-qlora) æ•°æ®é›†åˆ†åˆ«å¯¹ LLaMA2-7Bï¼ˆ**21 åˆ†é’Ÿå†…**ï¼‰å’Œ LLaMA2-70Bï¼ˆ**3.14 å°æ—¶å†…**ï¼‰è¿›è¡Œäº†å¾®è°ƒï¼Œå…·ä½“è¯¦æƒ…å‚é˜…[åšå®¢](https://www.intel.com/content/www/us/en/developer/articles/technical/finetuning-llms-on-intel-gpus-using-bigdl-llm.html)ã€‚ 
- [2023/12] `ipex-llm` ç°åœ¨æ”¯æŒ [ReLoRA](../../python/llm/example/GPU/LLM-Finetuning/ReLora) (å…·ä½“å†…å®¹è¯·å‚é˜… *["ReLoRA: High-Rank Training Through Low-Rank Updates"](https://arxiv.org/abs/2307.05695)*).
- [2023/12] `ipex-llm` ç°åœ¨åœ¨ Intel [GPU](../../python/llm/example/GPU/HuggingFace/LLM/mixtral) å’Œ [CPU](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/mixtral) ä¸Šå‡æ”¯æŒ [Mixtral-8x7B](../../python/llm/example/GPU/HuggingFace/LLM/mixtral)ã€‚
- [2023/12] `ipex-llm` ç°åœ¨æ”¯æŒ [QA-LoRA](../../python/llm/example/GPU/LLM-Finetuning/QA-LoRA) (å…·ä½“å†…å®¹è¯·å‚é˜… *["QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models"](https://arxiv.org/abs/2309.14717)*). 
- [2023/12] `ipex-llm` ç°åœ¨åœ¨ Intel ***GPU*** ä¸Šæ”¯æŒ [FP8 and FP4 inference](../../python/llm/example/GPU/HuggingFace/More-Data-Types)ã€‚
- [2023/11] åˆæ­¥æ”¯æŒç›´æ¥å°† [GGUF](../../python/llm/example/GPU/HuggingFace/Advanced-Quantizations/GGUF)ï¼Œ[AWQ](../../python/llm/example/GPU/HuggingFace/Advanced-Quantizations/AWQ) å’Œ [GPTQ](../../python/llm/example/GPU/HuggingFace/Advanced-Quantizations/GPTQ) æ¨¡å‹åŠ è½½åˆ° `ipex-llm` ä¸­ã€‚
- [2023/11] `ipex-llm` ç°åœ¨åœ¨ Intel [GPU](../../python/llm/example/GPU/vLLM-Serving) å’Œ [CPU](../../python/llm/example/CPU/vLLM-Serving) ä¸Šéƒ½æ”¯æŒ [vLLM continuous batching](../../python/llm/example/GPU/vLLM-Serving) ã€‚
- [2023/10] `ipex-llm` ç°åœ¨åœ¨ Intel [GPU](../../python/llm/example/GPU/LLM-Finetuning/QLoRA) å’Œ [CPU](../../python/llm/example/CPU/QLoRA-FineTuning) ä¸Šå‡æ”¯æŒ [QLoRA finetuning](../../python/llm/example/GPU/LLM-Finetuning/QLoRA) ã€‚
- [2023/10] `ipex-llm` ç°åœ¨åœ¨ Intel GPU å’Œ CPU ä¸Šéƒ½æ”¯æŒ [FastChat serving](../../python/llm/src/ipex_llm/llm/serving) ã€‚
- [2023/09] `ipex-llm` ç°åœ¨æ”¯æŒ [Intel GPU](../../python/llm/example/GPU) (åŒ…æ‹¬ iGPU, Arc, Flex å’Œ MAX)ã€‚
- [2023/09] `ipex-llm` [æ•™ç¨‹](https://github.com/intel-analytics/ipex-llm-tutorial) å·²å‘å¸ƒã€‚
 
</details> 

## `ipex-llm` å¿«é€Ÿå…¥é—¨

### ä½¿ç”¨
- [Ollama Portable Zip](Quickstart/ollama_portable_zip_quickstart.zh-CN.md): åœ¨ Intel GPU ä¸Šç›´æ¥**å…å®‰è£…è¿è¡Œ Ollama**ã€‚
- [Arc B580](Quickstart/bmg_quickstart.md): åœ¨ Intel Arc **B580** GPU ä¸Šè¿è¡Œ `ipex-llm`ï¼ˆåŒ…æ‹¬ Ollama, llama.cpp, PyTorch, HuggingFace ç­‰ï¼‰
- [NPU](Quickstart/npu_quickstart.md): åœ¨ Intel **NPU** ä¸Šè¿è¡Œ `ipex-llm`ï¼ˆæ”¯æŒ Python å’Œ C++ï¼‰
- [llama.cpp](Quickstart/llama_cpp_quickstart.zh-CN.md): åœ¨ Intel GPU ä¸Šè¿è¡Œ **llama.cpp** (*ä½¿ç”¨ `ipex-llm` çš„ C++ æ¥å£*) 
- [Ollama](Quickstart/ollama_quickstart.zh-CN.md): åœ¨ Intel GPU ä¸Šè¿è¡Œ **ollama** (*ä½¿ç”¨ `ipex-llm` çš„ C++ æ¥å£*) 
- [PyTorch/HuggingFace](Quickstart/install_windows_gpu.zh-CN.md): ä½¿ç”¨ [Windows](Quickstart/install_windows_gpu.zh-CN.md) å’Œ [Linux](Quickstart/install_linux_gpu.zh-CN.md) åœ¨ Intel GPU ä¸Šè¿è¡Œ **PyTorch**ã€**HuggingFace**ã€**LangChain**ã€**LlamaIndex** ç­‰ (*ä½¿ç”¨ `ipex-llm` çš„ Python æ¥å£*) 
- [vLLM](Quickstart/vLLM_quickstart.md): åœ¨ Intel [GPU](DockerGuides/vllm_docker_quickstart.md) å’Œ [CPU](DockerGuides/vllm_cpu_docker_quickstart.md) ä¸Šä½¿ç”¨ `ipex-llm` è¿è¡Œ **vLLM** 
- [FastChat](Quickstart/fastchat_quickstart.md): åœ¨ Intel GPU å’Œ CPU ä¸Šä½¿ç”¨ `ipex-llm` è¿è¡Œ **FastChat** æœåŠ¡
- [Serving on multiple Intel GPUs](Quickstart/deepspeed_autotp_fastapi_quickstart.md): åˆ©ç”¨ DeepSpeed AutoTP å’Œ FastAPI åœ¨ **å¤šä¸ª Intel GPU** ä¸Šè¿è¡Œ `ipex-llm` æ¨ç†æœåŠ¡
- [Text-Generation-WebUI](Quickstart/webui_quickstart.md): ä½¿ç”¨ `ipex-llm` è¿è¡Œ `oobabooga` **WebUI** 
- [Axolotl](Quickstart/axolotl_quickstart.md): ä½¿ç”¨ **Axolotl** å’Œ `ipex-llm` è¿›è¡Œ LLM å¾®è°ƒ
- [Benchmarking](Quickstart/benchmark_quickstart.md):  åœ¨ Intel GPU å’Œ CPU ä¸Šè¿è¡Œ**æ€§èƒ½åŸºå‡†æµ‹è¯•**ï¼ˆå»¶è¿Ÿå’Œååé‡ï¼‰
 
### Docker
- [GPU Inference in C++](DockerGuides/docker_cpp_xpu_quickstart.md): åœ¨ Intel GPU ä¸Šä½¿ç”¨ `ipex-llm` è¿è¡Œ `llama.cpp`, `ollama`ç­‰
- [GPU Inference in Python](DockerGuides/docker_pytorch_inference_gpu.md) : åœ¨ Intel GPU ä¸Šä½¿ç”¨ `ipex-llm` è¿è¡Œ HuggingFace `transformers`, `LangChain`, `LlamaIndex`, `ModelScope`ï¼Œç­‰
- [vLLM on GPU](DockerGuides/vllm_docker_quickstart.md): åœ¨ Intel GPU ä¸Šä½¿ç”¨ `ipex-llm` è¿è¡Œ `vLLM` æ¨ç†æœåŠ¡
- [vLLM on CPU](DockerGuides/vllm_cpu_docker_quickstart.md): åœ¨ Intel CPU ä¸Šä½¿ç”¨ `ipex-llm` è¿è¡Œ `vLLM` æ¨ç†æœåŠ¡  
- [FastChat on GPU](DockerGuides/fastchat_docker_quickstart.md): åœ¨ Intel GPU ä¸Šä½¿ç”¨ `ipex-llm` è¿è¡Œ `FastChat` æ¨ç†æœåŠ¡
- [VSCode on GPU](DockerGuides/docker_run_pytorch_inference_in_vscode.md): åœ¨ Intel GPU ä¸Šä½¿ç”¨ VSCode å¼€å‘å¹¶è¿è¡ŒåŸºäº Python çš„ `ipex-llm` åº”ç”¨

### åº”ç”¨
- [GraphRAG](Quickstart/graphrag_quickstart.md): åŸºäº `ipex-llm` ä½¿ç”¨æœ¬åœ° LLM è¿è¡Œ Microsoft çš„ `GraphRAG`
- [RAGFlow](Quickstart/ragflow_quickstart.md): åŸºäº `ipex-llm` è¿è¡Œ `RAGFlow` (*ä¸€ä¸ªå¼€æºçš„ RAG å¼•æ“*)  
- [LangChain-Chatchat](Quickstart/chatchat_quickstart.md): åŸºäº `ipex-llm` è¿è¡Œ `LangChain-Chatchat` (*ä½¿ç”¨ RAG pipline çš„çŸ¥è¯†é—®ç­”åº“*)
- [Coding copilot](Quickstart/continue_quickstart.md): åŸºäº `ipex-llm` è¿è¡Œ `Continue` (VSCode é‡Œçš„ç¼–ç æ™ºèƒ½åŠ©æ‰‹)
- [Open WebUI](Quickstart/open_webui_with_ollama_quickstart.md): åŸºäº `ipex-llm` è¿è¡Œ `Open WebUI`
- [PrivateGPT](Quickstart/privateGPT_quickstart.md): åŸºäº `ipex-llm` è¿è¡Œ `PrivateGPT` ä¸æ–‡æ¡£è¿›è¡Œäº¤äº’
- [Dify platform](Quickstart/dify_quickstart.md): åœ¨`Dify`(*ä¸€æ¬¾å¼€æºçš„å¤§è¯­è¨€æ¨¡å‹åº”ç”¨å¼€å‘å¹³å°*) é‡Œæ¥å…¥ `ipex-llm` åŠ é€Ÿæœ¬åœ° LLM

### å®‰è£…
- [Windows GPU](Quickstart/install_windows_gpu.zh-CN.md): åœ¨å¸¦æœ‰ Intel GPU çš„ Windows ç³»ç»Ÿä¸Šå®‰è£… `ipex-llm` 
- [Linux GPU](Quickstart/install_linux_gpu.zh-CN.md): åœ¨å¸¦æœ‰ Intel GPU çš„Linuxç³»ç»Ÿä¸Šå®‰è£… `ipex-llm` 
- *æ›´å¤šå†…å®¹, è¯·å‚è€ƒ[å®Œæ•´å®‰è£…æŒ‡å—](Overview/install.md)*

### ä»£ç ç¤ºä¾‹
- #### ä½æ¯”ç‰¹æ¨ç†
  - [INT4 inference](../../python/llm/example/GPU/HuggingFace/LLM): åœ¨ Intel [GPU](../../python/llm/example/GPU/HuggingFace/LLM) å’Œ [CPU](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model) ä¸Šè¿›è¡Œ **INT4** LLM æ¨ç†
  - [FP8/FP6/FP4 inference](../../python/llm/example/GPU/HuggingFace/More-Data-Types): åœ¨ Intel [GPU](../../python/llm/example/GPU/HuggingFace/More-Data-Types) ä¸Šè¿›è¡Œ **FP8**ï¼Œ**FP6** å’Œ **FP4** LLM æ¨ç†
  - [INT8 inference](../../python/llm/example/GPU/HuggingFace/More-Data-Types): åœ¨ Intel [GPU](../../python/llm/example/GPU/HuggingFace/More-Data-Types) å’Œ [CPU](../../python/llm/example/CPU/HF-Transformers-AutoModels/More-Data-Types) ä¸Šè¿›è¡Œ **INT8** LLM æ¨ç† 
  - [INT2 inference](../../python/llm/example/GPU/HuggingFace/Advanced-Quantizations/GGUF-IQ2): åœ¨ Intel [GPU](../../python/llm/example/GPU/HuggingFace/Advanced-Quantizations/GGUF-IQ2) ä¸Šè¿›è¡Œ **INT2** LLM æ¨ç† (åŸºäº llama.cpp IQ2 æœºåˆ¶) 
- #### FP16/BF16 æ¨ç†
  - åœ¨ Intel [GPU](../../python/llm/example/GPU/Speculative-Decoding) ä¸Šè¿›è¡Œ **FP16** LLM æ¨ç†ï¼ˆå¹¶ä½¿ç”¨ [self-speculative decoding](Inference/Self_Speculative_Decoding.md) ä¼˜åŒ–ï¼‰
  - åœ¨ Intel [CPU](../../python/llm/example/CPU/Speculative-Decoding) ä¸Šè¿›è¡Œ **BF16** LLM æ¨ç†ï¼ˆå¹¶ä½¿ç”¨ [self-speculative decoding](Inference/Self_Speculative_Decoding.md) ä¼˜åŒ–ï¼‰
- #### åˆ†å¸ƒå¼æ¨ç†
  - åœ¨ Intel [GPU](../../python/llm/example/GPU/Pipeline-Parallel-Inference) ä¸Šè¿›è¡Œ **æµæ°´çº¿å¹¶è¡Œ** æ¨ç†
  - åœ¨ Intel [GPU](../../python/llm/example/GPU/Deepspeed-AutoTP) ä¸Šè¿›è¡Œ **DeepSpeed AutoTP** æ¨ç†
- #### ä¿å­˜å’ŒåŠ è½½
  - [Low-bit models](../../python/llm/example/CPU/HF-Transformers-AutoModels/Save-Load): ä¿å­˜å’ŒåŠ è½½ `ipex-llm` ä½æ¯”ç‰¹æ¨¡å‹ (INT4/FP4/FP6/INT8/FP8/FP16/etc.)
  - [GGUF](../../python/llm/example/GPU/HuggingFace/Advanced-Quantizations/GGUF): ç›´æ¥å°† GGUF æ¨¡å‹åŠ è½½åˆ° `ipex-llm` ä¸­
  - [AWQ](../../python/llm/example/GPU/HuggingFace/Advanced-Quantizations/AWQ): ç›´æ¥å°† AWQ æ¨¡å‹åŠ è½½åˆ° `ipex-llm` ä¸­
  - [GPTQ](../../python/llm/example/GPU/HuggingFace/Advanced-Quantizations/GPTQ): ç›´æ¥å°† GPTQ æ¨¡å‹åŠ è½½åˆ° `ipex-llm` ä¸­
- #### å¾®è°ƒ
  - åœ¨ Intel [GPU](../../python/llm/example/GPU/LLM-Finetuning) è¿›è¡Œ LLM å¾®è°ƒï¼ŒåŒ…æ‹¬ [LoRA](../../python/llm/example/GPU/LLM-Finetuning/LoRA)ï¼Œ[QLoRA](../../python/llm/example/GPU/LLM-Finetuning/QLoRA)ï¼Œ[DPO](../../python/llm/example/GPU/LLM-Finetuning/DPO)ï¼Œ[QA-LoRA](../../python/llm/example/GPU/LLM-Finetuning/QA-LoRA) å’Œ [ReLoRA](../../python/llm/example/GPU/LLM-Finetuning/ReLora)
  - åœ¨ Intel [CPU](../../python/llm/example/CPU/QLoRA-FineTuning) è¿›è¡Œ QLoRA å¾®è°ƒ 
- #### ä¸ç¤¾åŒºåº“é›†æˆ
  - [HuggingFace transformers](../../python/llm/example/GPU/HuggingFace)
  - [Standard PyTorch model](../../python/llm/example/GPU/PyTorch-Models)
  - [LangChain](../../python/llm/example/GPU/LangChain)
  - [LlamaIndex](../../python/llm/example/GPU/LlamaIndex)
  - [DeepSpeed-AutoTP](../../python/llm/example/GPU/Deepspeed-AutoTP)
  - [Axolotl](Quickstart/axolotl_quickstart.md)
  - [HuggingFace PEFT](../../python/llm/example/GPU/LLM-Finetuning/HF-PEFT)
  - [HuggingFace TRL](../../python/llm/example/GPU/LLM-Finetuning/DPO)
  - [AutoGen](../../python/llm/example/CPU/Applications/autogen)
  - [ModeScope](../../python/llm/example/GPU/ModelScope-Models)
- [æ•™ç¨‹](https://github.com/intel-analytics/ipex-llm-tutorial)

## API æ–‡æ¡£
- [HuggingFace Transformers å…¼å®¹çš„ API (Auto Classes)](PythonAPI/transformers.md)
- [é€‚ç”¨äºä»»æ„ Pytorch æ¨¡å‹çš„ API](https://github.com/intel-analytics/ipex-llm/blob/main/PythonAPI/optimize.md)

## FAQ
- [å¸¸è§é—®é¢˜è§£ç­”](Overview/FAQ/faq.md)

## æ¨¡å‹éªŒè¯
50+ æ¨¡å‹å·²ç»åœ¨ `ipex-llm` ä¸Šå¾—åˆ°ä¼˜åŒ–å’ŒéªŒè¯ï¼ŒåŒ…æ‹¬ *LLaMA/LLaMA2, Mistral, Mixtral, Gemma, LLaVA, Whisper, ChatGLM2/ChatGLM3, Baichuan/Baichuan2, Qwen/Qwen-1.5, InternLM,* æ›´å¤šæ¨¡å‹è¯·å‚çœ‹ä¸‹è¡¨ï¼Œ
  
| æ¨¡å‹       | CPU ç¤ºä¾‹                                  | GPU ç¤ºä¾‹                                  | NPU ç¤ºä¾‹                                  |
|----------- |------------------------------------------|-------------------------------------------|-------------------------------------------|
| LLaMA  | [link1](../../python/llm/example/CPU/Native-Models), [link2](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/vicuna) |[link](../../python/llm/example/GPU/HuggingFace/LLM/vicuna)|
| LLaMA 2    | [link1](../../python/llm/example/CPU/Native-Models), [link2](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/llama2) | [link](../../python/llm/example/GPU/HuggingFace/LLM/llama2)  | [Python link](../../python/llm/example/NPU/HF-Transformers-AutoModels/LLM), [C++ link](../../python/llm/example/NPU/HF-Transformers-AutoModels/LLM/CPP_Examples) |
| LLaMA 3    | [link](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/llama3) | [link](../../python/llm/example/GPU/HuggingFace/LLM/llama3)  | [Python link](../../python/llm/example/NPU/HF-Transformers-AutoModels/LLM), [C++ link](../../python/llm/example/NPU/HF-Transformers-AutoModels/LLM/CPP_Examples) |
| LLaMA 3.1    | [link](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/llama3.1) | [link](../../python/llm/example/GPU/HuggingFace/LLM/llama3.1)  |
| LLaMA 3.2    |  | [link](../../python/llm/example/GPU/HuggingFace/LLM/llama3.2)  | [Python link](../../python/llm/example/NPU/HF-Transformers-AutoModels/LLM), [C++ link](../../python/llm/example/NPU/HF-Transformers-AutoModels/LLM/CPP_Examples) |
| LLaMA 3.2-Vision    |  | [link](../../python/llm/example/GPU/PyTorch-Models/Model/llama3.2-vision/)  |
| ChatGLM    | [link](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/chatglm)   |    | 
| ChatGLM2   | [link](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/chatglm2)  | [link](../../python/llm/example/GPU/HuggingFace/LLM/chatglm2)   |
| ChatGLM3   | [link](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/chatglm3)  | [link](../../python/llm/example/GPU/HuggingFace/LLM/chatglm3)   |
| GLM-4      | [link](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/glm4)      | [link](../../python/llm/example/GPU/HuggingFace/LLM/glm4)       |
| GLM-4V     | [link](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/glm-4v)    | [link](../../python/llm/example/GPU/HuggingFace/Multimodal/glm-4v)     |
| GLM-Edge   |  | [link](../../python/llm/example/GPU/HuggingFace/LLM/glm-edge) | [Python link](../../python/llm/example/NPU/HF-Transformers-AutoModels/LLM) |
| GLM-Edge-V   |  | [link](../../python/llm/example/GPU/HuggingFace/Multimodal/glm-edge-v) |
| Mistral    | [link](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/mistral)   | [link](../../python/llm/example/GPU/HuggingFace/LLM/mistral)    |
| Mixtral    | [link](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/mixtral)   | [link](../../python/llm/example/GPU/HuggingFace/LLM/mixtral)    |
| Falcon     | [link](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/falcon)    | [link](../../python/llm/example/GPU/HuggingFace/LLM/falcon)     |
| MPT        | [link](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/mpt)       | [link](../../python/llm/example/GPU/HuggingFace/LLM/mpt)        |
| Dolly-v1   | [link](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/dolly_v1)  | [link](../../python/llm/example/GPU/HuggingFace/LLM/dolly-v1)   | 
| Dolly-v2   | [link](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/dolly_v2)  | [link](../../python/llm/example/GPU/HuggingFace/LLM/dolly-v2)   | 
| Replit Code| [link](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/replit)    | [link](../../python/llm/example/GPU/HuggingFace/LLM/replit)     |
| RedPajama  | [link1](../../python/llm/example/CPU/Native-Models), [link2](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/redpajama) |    | 
| Phoenix    | [link1](../../python/llm/example/CPU/Native-Models), [link2](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/phoenix)   |    | 
| StarCoder  | [link1](../../python/llm/example/CPU/Native-Models), [link2](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/starcoder) | [link](../../python/llm/example/GPU/HuggingFace/LLM/starcoder) | 
| Baichuan   | [link](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/baichuan)  | [link](../../python/llm/example/GPU/HuggingFace/LLM/baichuan)   |
| Baichuan2  | [link](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/baichuan2) | [link](../../python/llm/example/GPU/HuggingFace/LLM/baichuan2)  | [Python link](../../python/llm/example/NPU/HF-Transformers-AutoModels/LLM) |
| InternLM   | [link](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/internlm)  | [link](../../python/llm/example/GPU/HuggingFace/LLM/internlm)   |
| InternVL2   |   | [link](../../python/llm/example/GPU/HuggingFace/Multimodal/internvl2)   |
| Qwen       | [link](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/qwen)      | [link](../../python/llm/example/GPU/HuggingFace/LLM/qwen)       |
| Qwen1.5 | [link](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/qwen1.5) | [link](../../python/llm/example/GPU/HuggingFace/LLM/qwen1.5) |
| Qwen2 | [link](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/qwen2) | [link](../../python/llm/example/GPU/HuggingFace/LLM/qwen2) | [Python link](../../python/llm/example/NPU/HF-Transformers-AutoModels/LLM), [C++ link](../../python/llm/example/NPU/HF-Transformers-AutoModels/LLM/CPP_Examples) |
| Qwen2.5 |  | [link](../../python/llm/example/GPU/HuggingFace/LLM/qwen2.5) | [Python link](../../python/llm/example/NPU/HF-Transformers-AutoModels/LLM), [C++ link](../../python/llm/example/NPU/HF-Transformers-AutoModels/LLM/CPP_Examples) |
| Qwen-VL    | [link](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/qwen-vl)   | [link](../../python/llm/example/GPU/HuggingFace/Multimodal/qwen-vl)    |
| Qwen2-VL    || [link](../../python/llm/example/GPU/PyTorch-Models/Model/qwen2-vl)    |
| Qwen2-Audio    |  | [link](../../python/llm/example/GPU/HuggingFace/Multimodal/qwen2-audio)    |
| Aquila     | [link](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/aquila)    | [link](../../python/llm/example/GPU/HuggingFace/LLM/aquila)     |
| Aquila2     | [link](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/aquila2)    | [link](../../python/llm/example/GPU/HuggingFace/LLM/aquila2)     |
| MOSS       | [link](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/moss)      |    | 
| Whisper    | [link](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/whisper)   | [link](../../python/llm/example/GPU/HuggingFace/Multimodal/whisper)    |
| Phi-1_5    | [link](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/phi-1_5)   | [link](../../python/llm/example/GPU/HuggingFace/LLM/phi-1_5)    |
| Flan-t5    | [link](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/flan-t5)   | [link](../../python/llm/example/GPU/HuggingFace/LLM/flan-t5)    |
| LLaVA      | [link](../../python/llm/example/CPU/PyTorch-Models/Model/llava)                 | [link](../../python/llm/example/GPU/PyTorch-Models/Model/llava)                  |
| CodeLlama  | [link](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/codellama) | [link](../../python/llm/example/GPU/HuggingFace/LLM/codellama)  |
| Skywork      | [link](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/skywork)                 |    |
| InternLM-XComposer  | [link](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/internlm-xcomposer)   |    |
| WizardCoder-Python | [link](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/wizardcoder-python) | |
| CodeShell | [link](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/codeshell) | |
| Fuyu      | [link](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/fuyu) | |
| Distil-Whisper | [link](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/distil-whisper) | [link](../../python/llm/example/GPU/HuggingFace/Multimodal/distil-whisper) |
| Yi | [link](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/yi) | [link](../../python/llm/example/GPU/HuggingFace/LLM/yi) |
| BlueLM | [link](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/bluelm) | [link](../../python/llm/example/GPU/HuggingFace/LLM/bluelm) |
| Mamba | [link](../../python/llm/example/CPU/PyTorch-Models/Model/mamba) | [link](../../python/llm/example/GPU/PyTorch-Models/Model/mamba) |
| SOLAR | [link](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/solar) | [link](../../python/llm/example/GPU/HuggingFace/LLM/solar) |
| Phixtral | [link](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/phixtral) | [link](../../python/llm/example/GPU/HuggingFace/LLM/phixtral) |
| InternLM2 | [link](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/internlm2) | [link](../../python/llm/example/GPU/HuggingFace/LLM/internlm2) |
| RWKV4 |  | [link](../../python/llm/example/GPU/HuggingFace/LLM/rwkv4) |
| RWKV5 |  | [link](../../python/llm/example/GPU/HuggingFace/LLM/rwkv5) |
| Bark | [link](../../python/llm/example/CPU/PyTorch-Models/Model/bark) | [link](../../python/llm/example/GPU/PyTorch-Models/Model/bark) |
| SpeechT5 |  | [link](../../python/llm/example/GPU/PyTorch-Models/Model/speech-t5) |
| DeepSeek-MoE | [link](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/deepseek-moe) |  |
| Ziya-Coding-34B-v1.0 | [link](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/ziya) | |
| Phi-2 | [link](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/phi-2) | [link](../../python/llm/example/GPU/HuggingFace/LLM/phi-2) |
| Phi-3 | [link](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/phi-3) | [link](../../python/llm/example/GPU/HuggingFace/LLM/phi-3) |
| Phi-3-vision | [link](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/phi-3-vision) | [link](../../python/llm/example/GPU/HuggingFace/Multimodal/phi-3-vision) |
| Yuan2 | [link](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/yuan2) | [link](../../python/llm/example/GPU/HuggingFace/LLM/yuan2) |
| Gemma | [link](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/gemma) | [link](../../python/llm/example/GPU/HuggingFace/LLM/gemma) |
| Gemma2 |  | [link](../../python/llm/example/GPU/HuggingFace/LLM/gemma2) |
| DeciLM-7B | [link](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/deciLM-7b) | [link](../../python/llm/example/GPU/HuggingFace/LLM/deciLM-7b) |
| Deepseek | [link](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/deepseek) | [link](../../python/llm/example/GPU/HuggingFace/LLM/deepseek) |
| StableLM | [link](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/stablelm) | [link](../../python/llm/example/GPU/HuggingFace/LLM/stablelm) |
| CodeGemma | [link](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/codegemma) | [link](../../python/llm/example/GPU/HuggingFace/LLM/codegemma) |
| Command-R/cohere | [link](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/cohere) | [link](../../python/llm/example/GPU/HuggingFace/LLM/cohere) |
| CodeGeeX2 | [link](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/codegeex2) | [link](../../python/llm/example/GPU/HuggingFace/LLM/codegeex2) |
| MiniCPM | [link](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/minicpm) | [link](../../python/llm/example/GPU/HuggingFace/LLM/minicpm) | [Python link](../../python/llm/example/NPU/HF-Transformers-AutoModels/LLM), [C++ link](../../python/llm/example/NPU/HF-Transformers-AutoModels/LLM/CPP_Examples) |
| MiniCPM3 |  | [link](../../python/llm/example/GPU/HuggingFace/LLM/minicpm3) |
| MiniCPM-V |  | [link](../../python/llm/example/GPU/HuggingFace/Multimodal/MiniCPM-V) |
| MiniCPM-V-2 | [link](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/minicpm-v-2) | [link](../../python/llm/example/GPU/HuggingFace/Multimodal/MiniCPM-V-2) |
| MiniCPM-Llama3-V-2_5 |  | [link](../../python/llm/example/GPU/HuggingFace/Multimodal/MiniCPM-Llama3-V-2_5) | [Python link](../../python/llm/example/NPU/HF-Transformers-AutoModels/Multimodal) |
| MiniCPM-V-2_6 | [link](../../python/llm/example/CPU/HF-Transformers-AutoModels/Model/minicpm-v-2_6) | [link](../../python/llm/example/GPU/HuggingFace/Multimodal/MiniCPM-V-2_6) | [Python link](../../python/llm/example/NPU/HF-Transformers-AutoModels/Multimodal) |
| StableDiffusion | | [link](../../python/llm/example/GPU/HuggingFace/Multimodal/StableDiffusion) |
| Bce-Embedding-Base-V1 | | | [Python link](../../python/llm/example/NPU/HF-Transformers-AutoModels/Embedding) |
| Speech_Paraformer-Large | | | [Python link](../../python/llm/example/NPU/HF-Transformers-AutoModels/Multimodal) |
