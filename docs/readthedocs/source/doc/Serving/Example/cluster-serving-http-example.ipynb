{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will use tensorflow.keras package to create a keras image classification application using model MobileNetV2, and transfer the application to Cluster Serving step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Keras application\n",
    "We will first show an original Keras application, which download the data and preprocess it, then create the MobileNetV2 model to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Obtain data from url:\"https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\"\n",
    "zip_file = tf.keras.utils.get_file(origin=\"https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\",\n",
    "                                   fname=\"cats_and_dogs_filtered.zip\", extract=True)\n",
    "\n",
    "# Find the directory of validation set\n",
    "base_dir, _ = os.path.splitext(zip_file)\n",
    "test_dir = os.path.join(base_dir, 'validation')\n",
    "# Set images size to 160x160x3\n",
    "image_size = 160\n",
    "\n",
    "# Rescale all images by 1./255 and apply image augmentation\n",
    "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Flow images using generator to the test_generator\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "                test_dir,\n",
    "                target_size=(image_size, image_size),\n",
    "                batch_size=1,\n",
    "                class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the base model from the pre-trained model MobileNet V2\n",
    "IMG_SHAPE=(160,160,3)\n",
    "model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In keras, input could be ndarray, or generator. We could just use `model.predict(test_generator)`. But to simplify, here we just input the first record to model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.997349   0.         0.         ... 0.         0.96874905\n",
      "    0.        ]\n",
      "   [1.8385804  0.3380084  2.4926844  ... 0.         0.14267397\n",
      "    0.        ]\n",
      "   [0.         0.         3.576158   ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.0062952  0.         ... 0.         0.15311003\n",
      "    0.        ]\n",
      "   [0.         1.7324333  1.1691046  ... 0.         0.9847245\n",
      "    0.        ]\n",
      "   [0.         0.84404707 3.2351522  ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.3681116 ]\n",
      "   [0.         3.3440204  0.5372138  ... 0.         0.\n",
      "    0.79515934]\n",
      "   [0.         3.0932055  3.5937624  ... 0.         0.\n",
      "    0.66862965]\n",
      "   [0.         1.4007983  0.         ... 0.         0.\n",
      "    2.8901892 ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.73307323]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    2.9129057 ]\n",
      "   [0.         0.         0.6134901  ... 0.         0.\n",
      "    2.7102432 ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    1.8489733 ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.22623205]]]]\n"
     ]
    }
   ],
   "source": [
    "prediction=model.predict(test_generator.next()[0])\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now the Keras application is completed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export TensorFlow Saved Model\n",
    "Next, we transfer the application to Cluster Serving. The first step is to save the model to SavedModel format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/user/anaconda3/envs/rec/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: /tmp/transfer_learning_mobilenetv2/assets\n",
      "assets\tsaved_model.pb\tvariables\n"
     ]
    }
   ],
   "source": [
    "# Save trained model to ./transfer_learning_mobilenetv2\n",
    "model.save('/tmp/transfer_learning_mobilenetv2')\n",
    "! ls /tmp/transfer_learning_mobilenetv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy Cluster Serving\n",
    "After model prepared, we start to deploy it on Cluster Serving.\n",
    "\n",
    "First install Cluster Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bigdl-serving in /home/user/anaconda3/envs/rec/lib/python3.6/site-packages (0.9.0)\n",
      "Requirement already satisfied: opencv-python in /home/user/anaconda3/envs/rec/lib/python3.6/site-packages (from bigdl-serving) (4.5.1.48)\n",
      "Requirement already satisfied: httpx in /home/user/anaconda3/envs/rec/lib/python3.6/site-packages (from bigdl-serving) (0.16.1)\n",
      "Requirement already satisfied: pyarrow in /home/user/.local/lib/python3.6/site-packages (from bigdl-serving) (1.0.1)\n",
      "Requirement already satisfied: redis in /home/user/anaconda3/envs/rec/lib/python3.6/site-packages (from bigdl-serving) (3.5.3)\n",
      "Requirement already satisfied: pyyaml in /home/user/anaconda3/envs/rec/lib/python3.6/site-packages (from bigdl-serving) (5.4.1)\n",
      "Requirement already satisfied: rfc3986[idna2008]<2,>=1.3 in /home/user/anaconda3/envs/rec/lib/python3.6/site-packages (from httpx->bigdl-serving) (1.4.0)\n",
      "Requirement already satisfied: certifi in /home/user/anaconda3/envs/rec/lib/python3.6/site-packages (from httpx->bigdl-serving) (2020.12.5)\n",
      "Requirement already satisfied: httpcore==0.12.* in /home/user/anaconda3/envs/rec/lib/python3.6/site-packages (from httpx->bigdl-serving) (0.12.3)\n",
      "Requirement already satisfied: sniffio in /home/user/anaconda3/envs/rec/lib/python3.6/site-packages (from httpx->bigdl-serving) (1.2.0)\n",
      "Requirement already satisfied: h11==0.* in /home/user/anaconda3/envs/rec/lib/python3.6/site-packages (from httpcore==0.12.*->httpx->bigdl-serving) (0.12.0)\n",
      "Requirement already satisfied: contextvars>=2.1 in /home/user/anaconda3/envs/rec/lib/python3.6/site-packages (from sniffio->httpx->bigdl-serving) (2.4)\n",
      "Requirement already satisfied: immutables>=0.9 in /home/user/anaconda3/envs/rec/lib/python3.6/site-packages (from contextvars>=2.1->sniffio->httpx->bigdl-serving) (0.14)\n",
      "Requirement already satisfied: idna in /home/user/anaconda3/envs/rec/lib/python3.6/site-packages (from rfc3986[idna2008]<2,>=1.3->httpx->bigdl-serving) (2.10)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/user/anaconda3/envs/rec/lib/python3.6/site-packages (from opencv-python->bigdl-serving) (1.19.2)\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/home/user/anaconda3/envs/rec/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install bigdl-serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Serving has been properly set up.\n",
      "You did not specify BIGDL_VERSION, will download 0.9.0\n",
      "BIGDL_VERSION is 0.9.0\n",
      "BIGDL_VERSION is 0.12.1\n",
      "SPARK_VERSION is 2.4.3\n",
      "2.4\n",
      "--2021-02-07 10:01:46--  https://repo1.maven.org/maven2/com/intel/analytics/bigdl/bigdl-bigdl_0.12.1-spark_2.4.3/0.9.0/bigdl-bigdl_0.12.1-spark_2.4.3-0.9.0-serving.jar\n",
      "Resolving child-prc.intel.com (child-prc.intel.com)... You are installing Cluster Serving by pip, downloading...\n",
      "\n",
      "SIGHUP received.\n",
      "Redirecting output to ‘wget-log.2’.\n"
     ]
    }
   ],
   "source": [
    "# we go to a new directory and initialize the environment\n",
    "! mkdir cluster-serving\n",
    "os.chdir('cluster-serving')\n",
    "! cluster-serving-init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2150K .......... .......... .......... .......... ..........  0% 27.0K 5h37m\r\n",
      "  2200K .......... .......... .......... .......... ..........  0% 33.6K 5h36m\r\n",
      "  2250K .......... .......... .......... .......... ..........  0% 27.3K 5h37m\r\n",
      "  2300K .......... .......... .......... .......... ..........  0% 30.3K 5h36m\r\n",
      "  2350K .......... .......... .......... .......... ..........  0% 29.7K 5h36m\r\n",
      "  2400K .......... .......... .......... .......... ..........  0% 23.7K 5h38m\r\n",
      "  2450K .......... .......... .......... .......... ..........  0% 23.4K 5h39m\r\n",
      "  2500K .......... .......... .......... .......... ..........  0% 23.4K 5h41m\r\n",
      "  2550K .......... .......... .......... .......... ..........  0% 22.3K 5h43m\r\n",
      "  2600K .......... .......... .......... ....."
     ]
    }
   ],
   "source": [
    "! tail wget-log.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you encounter slow download issue like above, you can just use following command to download\n",
    "# ! wget https://repo1.maven.org/maven2/com/intel/analytics/bigdl/bigdl-bigdl_0.12.1-spark_2.4.3/0.9.0/bigdl-bigdl_0.12.1-spark_2.4.3-0.9.0-serving.jar\n",
    "\n",
    "# if you are using wget to download, call mv *serving.jar bigdl.jar again after downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.yaml  bigdl.jar\r\n"
     ]
    }
   ],
   "source": [
    "# After initialization finished, check the directory\n",
    "! ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We config the model path in `config.yaml` to following (the detail of config is at [Cluster Serving Configuration](https://github.com/intel-analytics/bigdl/blob/master/docs/docs/ClusterServingGuide/ProgrammingGuide.md#2-configuration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BigDL Cluster Serving\n",
    "\n",
    "model:\n",
    "  # model path must be provided\n",
    "  path: /tmp/transfer_learning_mobilenetv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## BigDL Cluster Serving\r\n",
      "\r\n",
      "model:\r\n",
      "  # model path must be provided\r\n",
      "  path: /tmp/transfer_learning_mobilenetv2\r\n",
      "  # name, default is serving_stream, you need to specify if running multiple servings\r\n",
      "  name:\r\n",
      "data:\r\n",
      "  # default, localhost:6379\r\n",
      "  src:\r\n"
     ]
    }
   ],
   "source": [
    "! head config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Cluster Serving\n",
    "\n",
    "Cluster Serving requires Flink and Redis installed, and corresponded environment variables set, check [Cluster Serving Installation Guide](https://github.com/intel-analytics/bigdl/blob/master/docs/docs/ClusterServingGuide/ProgrammingGuide.md#1-installation) for detail.\n",
    "\n",
    "Flink cluster should start before Cluster Serving starts, if Flink cluster is not started, call following to start a local Flink cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting cluster.\n",
      "Starting standalonesession daemon on host my-PC.\n",
      "Starting taskexecutor daemon on host my-PC.\n"
     ]
    }
   ],
   "source": [
    "! $FLINK_HOME/bin/start-cluster.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After configuration, start Cluster Serving by `cluster-serving-start` (the detail is at [Cluster Serving Programming Guide](https://github.com/intel-analytics/bigdl/blob/master/docs/docs/ClusterServingGuide/ProgrammingGuide.md#3-launching-service))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_path=\"/tmp/transfer_learning_mobilenetv2\"\n",
      "redis_timeout=\"5000\"\n",
      "Redis maxmemory is not set, using default value 8G\n",
      "redis server started, please check log in redis.log\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "redis config maxmemory set to 8G\n",
      "OK\n",
      "OK\n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/home/user/dep/flink-1.11.2/lib/bigdl-bigdl_0.12.0-spark_2.4.3-0.9.0-SNAPSHOT-serving.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/user/dep/flink-1.11.2/lib/log4j-slf4j-impl-2.12.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/user/dep/flink-1.11.2/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "log4j:WARN No appenders could be found for logger (org.apache.flink.client.cli.CliFrontend).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n",
      "Starting new Cluster Serving job.\n",
      "Cluster Serving job submitted, check log in log-cluster_serving-serving_stream.txt\n",
      "To list Cluster Serving job status, use cluster-serving-cli list\n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/home/user/dep/flink-1.11.2/lib/bigdl-bigdl_0.12.0-spark_2.4.3-0.9.0-SNAPSHOT-serving.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/user/dep/flink-1.11.2/lib/log4j-slf4j-impl-2.12.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/user/dep/flink-1.11.2/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "log4j:WARN No appenders could be found for logger (org.apache.flink.client.cli.CliFrontend).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n",
      "[Full GC (Metadata GC Threshold)  32304K->20432K(1030144K), 0.0213821 secs]\n"
     ]
    }
   ],
   "source": [
    "! cluster-serving-start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction using Cluster Serving\n",
    "Next we start Cluster Serving code at python client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "redis group exist, will not create new one\n",
      "redis group exist, will not create new one\n"
     ]
    }
   ],
   "source": [
    "from bigdl.serving.client import InputQueue, OutputQueue\n",
    "input_queue = InputQueue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Cluster Serving, only NdArray is supported as input. Thus, we first transform the generator to ndarray (If you do not know how to transform your input to NdArray, you may get help at [data transform guide](https://github.com/intel-analytics/bigdl/tree/master/docs/docs/ClusterServingGuide/OtherFrameworkUsers#data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.41176474, 0.50980395, 0.5882353 ],\n",
       "         [0.42352945, 0.47450984, 0.50980395],\n",
       "         [0.4901961 , 0.5058824 , 0.5019608 ],\n",
       "         ...,\n",
       "         [0.5764706 , 0.6392157 , 0.7019608 ],\n",
       "         [0.454902  , 0.5176471 , 0.5803922 ],\n",
       "         [0.3647059 , 0.427451  , 0.4784314 ]],\n",
       "\n",
       "        [[0.31764707, 0.38431376, 0.4156863 ],\n",
       "         [0.35686275, 0.38431376, 0.40784317],\n",
       "         [0.34509805, 0.34509805, 0.3529412 ],\n",
       "         ...,\n",
       "         [0.5803922 , 0.64705884, 0.6862745 ],\n",
       "         [0.48627454, 0.5529412 , 0.5921569 ],\n",
       "         [0.48235297, 0.54509807, 0.59607846]],\n",
       "\n",
       "        [[0.4039216 , 0.4431373 , 0.44705886],\n",
       "         [0.35686275, 0.36078432, 0.37647063],\n",
       "         [0.46274513, 0.4431373 , 0.47058827],\n",
       "         ...,\n",
       "         [0.53333336, 0.6       , 0.6313726 ],\n",
       "         [0.47450984, 0.5411765 , 0.5686275 ],\n",
       "         [0.5137255 , 0.5764706 , 0.627451  ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.44705886, 0.5019608 , 0.54509807],\n",
       "         [0.42352945, 0.48627454, 0.5372549 ],\n",
       "         [0.37647063, 0.43921572, 0.49803925],\n",
       "         ...,\n",
       "         [0.69411767, 0.69411767, 0.69411767],\n",
       "         [0.6745098 , 0.6745098 , 0.68235296],\n",
       "         [0.6392157 , 0.63529414, 0.6666667 ]],\n",
       "\n",
       "        [[0.3647059 , 0.41960788, 0.454902  ],\n",
       "         [0.35686275, 0.427451  , 0.47450984],\n",
       "         [0.3254902 , 0.3921569 , 0.454902  ],\n",
       "         ...,\n",
       "         [0.5647059 , 0.5647059 , 0.5647059 ],\n",
       "         [0.627451  , 0.627451  , 0.63529414],\n",
       "         [0.7176471 , 0.70980394, 0.76470596]],\n",
       "\n",
       "        [[0.34117648, 0.40784317, 0.43529415],\n",
       "         [0.29803923, 0.37254903, 0.427451  ],\n",
       "         [0.31764707, 0.3921569 , 0.45882356],\n",
       "         ...,\n",
       "         [0.454902  , 0.454902  , 0.46274513],\n",
       "         [0.5803922 , 0.57254905, 0.6156863 ],\n",
       "         [0.5137255 , 0.5019608 , 0.58431375]]]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = test_generator.next()[0]\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to Redis successful\n",
      "redis group exist, will not create new one\n",
      "Write to Redis successful\n"
     ]
    }
   ],
   "source": [
    "# Use async api to put and get, you have pass a name arg and use the name to get\n",
    "input_queue.enqueue('my-input', t=arr)\n",
    "output_queue = OutputQueue()\n",
    "prediction = output_queue.query('my-input')\n",
    "# Use sync api to predict, this will block until the result is get or timeout\n",
    "prediction = input_queue.predict(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 1.3543907 ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 4.1898136 ,\n",
       "         0.        , 0.        ]],\n",
       "\n",
       "       [[0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 3.286649  , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 4.0817494 , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 3.3224926 , 0.        , ..., 1.4220613 ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 4.9100547 ,\n",
       "         0.        , 0.        ]],\n",
       "\n",
       "       [[0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 1.5577714 , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 1.767426  , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 2.3534465 , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.21401057,\n",
       "         0.        , 0.        ]],\n",
       "\n",
       "       [[0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.34797698, 0.        ],\n",
       "        [0.        , 1.4496232 , 0.        , ..., 0.        ,\n",
       "         1.6221215 , 0.        ],\n",
       "        [0.        , 0.6171873 , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ]],\n",
       "\n",
       "       [[0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         1.192298  , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ]]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything works well, the result `prediction` would be the exactly the same NdArray object with the output of original Keras model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is the way to use http service through python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the http server via jar\n",
    "# ! java -jar bigdl-bigdl_0.10.0-spark_2.4.3-0.9.0-SNAPSHOT-http.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do not know how to find the jar or other http service, you may get help at [Cluster Serving http guide](https://github.com/intel-analytics/bigdl/blob/master/docs/docs/ClusterServingGuide/ProgrammingGuide.md#3-launching-service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "welcome to BigDL web serving frontend"
     ]
    }
   ],
   "source": [
    "! curl http://localhost:10020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster Serving provides an Python util `http_response_to_ndarray` which let user parse http response directly to ndarray, as following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "from bigdl.serving.client import http_response_to_ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ]],\n",
       "\n",
       "       [[0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ]],\n",
       "\n",
       "       [[0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ]],\n",
       "\n",
       "       [[0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.7070324 , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 1.9520156 , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ]],\n",
       "\n",
       "       [[0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.45007578],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'http://localhost:10020/predict'\n",
    "d = json.dumps({\"instances\":[{\"floatTensor\": arr.tolist()}]})\n",
    "r = requests.post(url, data=d)\n",
    "\n",
    "http_prediction = http_response_to_ndarray(r)\n",
    "http_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't forget to delete the model you save for this tutorial\n",
    "! rm -rf /tmp/transfer_learning_mobilenetv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the end of this tutorial. If you have any question, you could raise an issue at [BigDL Github](https://github.com/intel-analytics/bigdl/issues)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
