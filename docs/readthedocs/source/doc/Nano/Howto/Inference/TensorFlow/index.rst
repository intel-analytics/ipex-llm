Inference Optimization: For TensorFlow Users
=============================================

* `How to accelerate a TensorFlow inference pipeline through ONNXRuntime <accelerate_tensorflow_inference_onnx.html>`_
* `How to accelerate a TensorFlow inference pipeline through OpenVINO <accelerate_tensorflow_inference_openvino.html>`_
* `How to conduct BFloat16 Mixed Precision inference in a TensorFlow Keras application <tensorflow_inference_bf16.html>`_
* `How to save and load optimized ONNXRuntime model in TensorFlow <tensorflow_save_and_load_onnx.html>`_
* `How to save and load optimized OpenVINO model in TensorFlow <tensorflow_save_and_load_openvino.html>`_
* `How to quantize your Tensorflow model for inference using Intel Neural Compressor <quantize_tensorflow_inference_inc.html>`_
* `How to quantize your Tensorflow model for inference using Accelerator <quantize_tensorflow_inference_acc.html>`_
* `How to quantize your Tensorflow model for inference by specifying accuracy control <quantize_tensorflow_accuracy_control.html>`_
* `How to find accelerated method with minimal latency for TensorFlow model using InferenceOptimizer <tensorflow_inference_optimizer_optimize.html>`_